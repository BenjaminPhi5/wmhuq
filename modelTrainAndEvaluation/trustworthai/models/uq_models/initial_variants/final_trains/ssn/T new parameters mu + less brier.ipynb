{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66016330-97f9-49f0-8637-099c21ac5cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strawberry\n"
     ]
    }
   ],
   "source": [
    "print(\"strawberry\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# dataset\n",
    "from twaidata.torchdatasets.in_ram_ds import MRISegmentation2DDataset, MRISegmentation3DDataset\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "\n",
    "# model\n",
    "from trustworthai.models.uq_models.initial_variants.HyperMapp3r_deterministic import HyperMapp3r\n",
    "from trustworthai.models.uq_models.initial_variants.HyperMapp3r_DDU import HyperMapp3rDDU\n",
    "from trustworthai.models.uq_models.initial_variants.HyperMapp3r_SSN import HyperMapp3rSSN\n",
    "\n",
    "\n",
    "# augmentation and pretrain processing\n",
    "from trustworthai.utils.augmentation.standard_transforms import RandomFlip, GaussianBlur, GaussianNoise, \\\n",
    "                                                            RandomResizeCrop, RandomAffine, \\\n",
    "                                                            NormalizeImg, PairedCompose, LabelSelect, \\\n",
    "                                                            PairedCentreCrop, CropZDim\n",
    "# loss function\n",
    "from trustworthai.utils.losses_and_metrics.per_individual_losses import (\n",
    "    log_cosh_dice_loss,\n",
    "    TverskyLoss,\n",
    "    FocalTverskyLoss,\n",
    "    DiceLossMetric\n",
    ")\n",
    "from torch.nn import BCELoss, MSELoss, BCEWithLogitsLoss\n",
    "\n",
    "# fitter\n",
    "from trustworthai.utils.fitting_and_inference.fitters.basic_lightning_fitter import StandardLitModelWrapper\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# misc\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import argparse\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchmetrics import Metric\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46522f74-a209-4f46-abc4-7510d9c2fdb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbba41f4a70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1307)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a436e24b-e9e3-4bcf-a1a5-a51b8aa39c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    transforms = [\n",
    "        LabelSelect(label_id=1),\n",
    "        RandomFlip(p=0.5, orientation=\"horizontal\"),\n",
    "        # GaussianBlur(p=0.5, kernel_size=7, sigma=(.1, 1.5)),\n",
    "        # GaussianNoise(p=0.2, mean=0, sigma=0.2),\n",
    "        RandomAffine(p=0.2, shear=(-18,18)),\n",
    "        RandomAffine(p=0.2, degrees=15),\n",
    "        RandomAffine(p=0.2, translate=(-0.1,0.1)),\n",
    "        RandomAffine(p=0.2, scale=(0.9, 1.1)),\n",
    "#         #RandomResizeCrop(p=1., scale=(0.6, 1.), ratio=(3./4., 4./3.))\n",
    "\n",
    "#         #RandomResizeCrop(p=1., scale=(0.3, 0.5), ratio=(3./4., 4./3.)) # ssn\n",
    "            \n",
    "    ]\n",
    "    transforms.append(lambda x, y: (x, y.squeeze().type(torch.long)))\n",
    "    return PairedCompose(transforms)\n",
    "\n",
    "def none_transform():\n",
    "    transforms = [\n",
    "        LabelSelect(label_id=1),\n",
    "        lambda x, y: (x, y.squeeze().type(torch.long))\n",
    "    ]\n",
    "    return PairedCompose(transforms)\n",
    "\n",
    "def train_val_test_split(dataset, val_prop, test_prop, seed):\n",
    "        # I think the sklearn version might be prefereable for determinism and things\n",
    "        # but that involves fiddling with the dataset implementation I think....\n",
    "        size = len(dataset)\n",
    "        test_size = int(test_prop*size) \n",
    "        val_size = int(val_prop*size)\n",
    "        train_size = size - val_size - test_size\n",
    "        train, val, test = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(seed))\n",
    "        return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "581552a2-11fb-4fbb-80f4-37e6d043aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from trustworthai.models.uq_models.drop_UNet import normalization_layer\n",
    "import torch.nn.functional as F\n",
    "from trustworthai.models.uq_models.initial_variants.HyperMapp3r_deterministic import HyperMapp3r\n",
    "import torch.distributions as td\n",
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b480ad28-8e8d-4e4b-9305-d3024aaa1ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = None\n",
    "is3D = False\n",
    "root_dir = \"/disk/scratch/s2208943/ipdis/preprep/out_data/collated/\"\n",
    "#root_dir = \"/media/benp/NVMEspare/datasets/preprocessing_attempts/local_results/collated/\"\n",
    "wmh_dir = root_dir + \"WMH_challenge_dataset/\"\n",
    "ed_dir = root_dir + \"EdData/\"\n",
    "\n",
    "domains = [ed_dir + d for d in [\"domainA\", \"domainB\", \"domainC\", \"domainD\"]]\n",
    "# domains = [ wmh_dir + d for d in ['Singapore', 'GE3T', 'Utrecht']]\n",
    "\n",
    "train_proportion = 0.7\n",
    "test_proportion = 0.1\n",
    "validation_proportion = 0.2\n",
    "seed = 3407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35a3c6cf-84b5-4f1e-8c35-3f181101dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3d to 2d dataset\n",
    "class MRISegDataset2DFrom3D(Dataset):\n",
    "    def __init__(self, dataset3D, transforms=None):\n",
    "        # calculate total number of slices (note need to iterate through every item\n",
    "        # because each image may have a different number of slices\n",
    "        size = 0\n",
    "        for data in dataset3D:\n",
    "            x = data[0]\n",
    "            size += x.shape[1]\n",
    "            \n",
    "        self.size = size\n",
    "        self.dataset3D = dataset3D\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        idx_to_scan_index = []\n",
    "        scan_starting_index = []\n",
    "        \n",
    "        scan_count = 0\n",
    "        starting_index = 0\n",
    "        for (ind, _) in dataset3D:\n",
    "            d_size = ind.shape[1] # slices are the second dim of 3D scan\n",
    "            idx_to_scan_index.append(torch.ones(d_size) * scan_count)\n",
    "            scan_starting_index.append(starting_index)\n",
    "            \n",
    "            scan_count += 1\n",
    "            starting_index += d_size\n",
    "            \n",
    "        self.idx_to_scan = torch.cat(idx_to_scan_index, dim=0).type(torch.int32)\n",
    "        # print(self.idx_to_scan.shape)\n",
    "        self.scan_starting_index = scan_starting_index\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # find out which scan to use\n",
    "        scan_idx = self.idx_to_scan[idx]\n",
    "        # get that dataset\n",
    "        scan_img, scan_label = self.dataset3D[scan_idx]\n",
    "        # find out where the element is in that dataset\n",
    "        item_idx = idx - self.scan_starting_index[scan_idx]\n",
    "        \n",
    "        #print(scan_img.shape, scan_label.shape)\n",
    "        slice_x = scan_img[:, item_idx]\n",
    "        slice_y = scan_label[:, item_idx] # slices are the second dim of a 3D scan (its channels, z, x, y for 3D scans)\n",
    "        \n",
    "        if self.transforms:\n",
    "            slice_x, slice_y = self.transforms(slice_x, slice_y)\n",
    "        \n",
    "        return slice_x, slice_y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afb26101-3bae-4801-95db-81127d60a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "### empty slice splitting\n",
    "class FilteredEmptyElementsDataset(Dataset):\n",
    "    def __init__(self, dataset, seed, transforms=None, empty_proportion_retained=0.1):\n",
    "        # print(len(dataset))\n",
    "        self.base_dataset = dataset\n",
    "        self.transforms = transforms\n",
    "        empty_indices = []\n",
    "        self.non_empty_indices = []\n",
    "        count = 0\n",
    "        for i, (x, y) in enumerate(dataset):\n",
    "            if y.sum() == 0:\n",
    "                count += 1\n",
    "                empty_indices.append(i)\n",
    "            else:\n",
    "                self.non_empty_indices.append(i)\n",
    "           \n",
    "        # print(count)\n",
    "        # print(len(self.non_empty_indices))\n",
    "        #print(count * empty_proportion_retained)\n",
    "                \n",
    "        # extract only a limited proportion of empty slices (take a random selection)\n",
    "        shuffled_indices = torch.randperm(count, generator=torch.Generator().manual_seed(seed))\n",
    "        emtpy_indices = torch.Tensor(empty_indices)\n",
    "        self.retained_empty_indices = torch.Tensor(empty_indices)[shuffled_indices[0:int(count * empty_proportion_retained)]]\n",
    "        self.size = len(self.non_empty_indices) + len(self.retained_empty_indices)\n",
    "        self.non_empty_size = len(self.non_empty_indices)\n",
    "        \n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.non_empty_size:\n",
    "            # select an empty slice\n",
    "            new_idx = self.retained_empty_indices[idx - self.non_empty_size]\n",
    "        else:\n",
    "            # select a slice with label in it\n",
    "            new_idx = self.non_empty_indices[idx]\n",
    "        new_idx = int(new_idx)\n",
    "        \n",
    "        img, label = self.base_dataset[new_idx]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img, label = self.transforms(img, label)\n",
    "            \n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b604dd-8578-4a41-b2c4-2cd5d3c86f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_domains = [MRISegmentation3DDataset(root_dir, domain, transforms=None) for domain in domains]\n",
    "\n",
    "# split into train, val test datasets\n",
    "datasets_3d = [train_val_test_split(dataset, validation_proportion, test_proportion, seed) for dataset in datasets_domains]\n",
    "\n",
    "# concat the train val test datsets\n",
    "train_dataset_3d = ConcatDataset([ds[0] for ds in datasets_3d])\n",
    "val_dataset_3d = ConcatDataset([ds[1] for ds in datasets_3d])\n",
    "test_dataset_3d = ConcatDataset([ds[2] for ds in datasets_3d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42b1953d-6c45-4cd9-8530-e9fe1b15e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_2d = [MRISegDataset2DFrom3D(ds, transforms=None) for ds in [train_dataset_3d, val_dataset_3d, test_dataset_3d]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27886bdf-2651-4824-b26e-55bd7bc9f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = [FilteredEmptyElementsDataset(ds, seed=seed, transforms=get_transforms()) for ds in datasets_2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d668d153-7de4-4e02-9e47-bff64f76aeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([224, 160])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab944a5e-c663-4113-ad12-7abbc66df80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 30, shuffle=False, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d971987-cfae-404b-80bd-7a92a9c076f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_class_prob(p_hat):\n",
    "    p_hat = torch.nn.functional.softmax(p_hat, dim=1)\n",
    "    p_hat = p_hat[:,1,:] # select class 0\n",
    "    return p_hat\n",
    "\n",
    "def individual_dice(p_hat, y_true):\n",
    "    p_hat = two_class_prob(p_hat)\n",
    "    s0 = p_hat.shape[0]\n",
    "    p_hat = p_hat.view(s0,-1)\n",
    "    y_true = y_true.view(s0,-1)\n",
    "    numerator = torch.sum(2. * p_hat * y_true, dim=1) + 1.\n",
    "    denominator = torch.sum(y_true + p_hat, dim=1) + 1.\n",
    "    combined = 1. - (numerator/denominator)\n",
    "    return combined\n",
    "    \n",
    "def dice_loss(p_hat, y_true):\n",
    "    combined = individual_dice(p_hat, y_true)\n",
    "    \n",
    "    # is empties\n",
    "    locs = torch.sum(y_true, dim=(-2, -1)) == 0\n",
    "    wheres = torch.where(locs)[0]\n",
    "    #print(wheres.shape)\n",
    "    # print(wheres)\n",
    "    #print(combined)\n",
    "    r = 0.5\n",
    "    combined[wheres] *= r\n",
    "    #print(combined)\n",
    "    \n",
    "    return torch.sum(combined) / ((y_true.shape[0] - wheres.shape[0]) + (wheres.shape[0] * r))\n",
    "\n",
    "def dice_loss_old(p_hat, y_true):\n",
    "    combined = individual_dice(p_hat, y_true)\n",
    "    return torch.mean(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70620e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from trustworthai.models.uq_models.drop_UNet import normalization_layer\n",
    "import torch.nn.functional as F\n",
    "from trustworthai.models.uq_models.initial_variants.HyperMapp3r_deterministic import HyperMapp3r\n",
    "import torch.distributions as td\n",
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e45c1f78-65e6-43b4-9403-88d655293f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_func(dims, transpose=False):\n",
    "    # determine convolution func\n",
    "        if dims == 2:\n",
    "            if transpose:\n",
    "                return nn.ConvTranspose2d\n",
    "            else:\n",
    "                return nn.Conv2d\n",
    "        elif dims == 3:\n",
    "            if transpose:\n",
    "                return nn.ConvTranspose3d\n",
    "            else:\n",
    "                return nn.Conv3d\n",
    "        else:\n",
    "            raise ValueError(f\"values of dims of 2 or 3 (2D or 2D conv) are supported only, not {dims}\")\n",
    "            \n",
    "def get_dropout_func(dims):\n",
    "    if dims == 2:\n",
    "        return nn.Dropout2d\n",
    "    if dims == 3:\n",
    "        return nn.Dropout3d\n",
    "    else:\n",
    "        return nn.Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abad93bf-b8ac-4a2d-b8f5-7f8a6009617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapedDistribution(td.Distribution):\n",
    "    def __init__(self, base_distribution: td.Distribution, new_event_shape: Tuple[int, ...]):\n",
    "        super().__init__(batch_shape=base_distribution.batch_shape, event_shape=new_event_shape, validate_args=False)\n",
    "        self.base_distribution = base_distribution\n",
    "        self.new_shape = base_distribution.batch_shape + new_event_shape\n",
    "        \n",
    "        #print(\"base distribution: \", self.base_distribution)\n",
    "\n",
    "    @property\n",
    "    def support(self):\n",
    "        return self.base_distribution.support\n",
    "\n",
    "    @property\n",
    "    def arg_constraints(self):\n",
    "        return self.base_distribution.arg_constraints()\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.base_distribution.mean.view(self.new_shape)\n",
    "\n",
    "    @property\n",
    "    def variance(self):\n",
    "        return self.base_distribution.variance.view(self.new_shape)\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size()):\n",
    "        return self.base_distribution.rsample(sample_shape).view(sample_shape + self.new_shape)\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        return self.base_distribution.log_prob(value.view(self.batch_shape + (-1,)))\n",
    "\n",
    "    def entropy(self):\n",
    "        return self.base_distribution.entropy()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29df04d4-4c0e-4973-bc10-12d18de8c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from torch.distributions.multivariate_normal import _batch_mahalanobis, _batch_mv\n",
    "from torch.distributions.utils import _standard_normal, lazy_property\n",
    "from pyro.distributions.torch_distribution import TorchDistribution\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "from torch.distributions.utils import lazy_property\n",
    "\n",
    "from pyro.distributions.torch import Chi2\n",
    "from pyro.distributions.torch_distribution import TorchDistribution\n",
    "from pyro.distributions.util import broadcast_shape\n",
    "\n",
    "def _batch_capacitance_tril(W, D):\n",
    "    r\"\"\"\n",
    "    Computes Cholesky of :math:`I + W.T @ inv(D) @ W` for a batch of matrices :math:`W`\n",
    "    and a batch of vectors :math:`D`.\n",
    "    \"\"\"\n",
    "    m = W.size(-1)\n",
    "    Wt_Dinv = W.mT / D.unsqueeze(-2)\n",
    "    K = torch.matmul(Wt_Dinv, W).contiguous()\n",
    "    K.view(-1, m * m)[:, ::m + 1] += 1  # add identity matrix to K\n",
    "    return torch.linalg.cholesky(K)\n",
    "\n",
    "\n",
    "def _batch_lowrank_logdet(W, D, capacitance_tril):\n",
    "    r\"\"\"\n",
    "    Uses \"matrix determinant lemma\"::\n",
    "        log|W @ W.T + D| = log|C| + log|D|,\n",
    "    where :math:`C` is the capacitance matrix :math:`I + W.T @ inv(D) @ W`, to compute\n",
    "    the log determinant.\n",
    "    \"\"\"\n",
    "    return 2 * capacitance_tril.diagonal(dim1=-2, dim2=-1).log().sum(-1) + D.log().sum(-1)\n",
    "\n",
    "\n",
    "def _batch_lowrank_mahalanobis(W, D, x, capacitance_tril):\n",
    "    r\"\"\"\n",
    "    Uses \"Woodbury matrix identity\"::\n",
    "        inv(W @ W.T + D) = inv(D) - inv(D) @ W @ inv(C) @ W.T @ inv(D),\n",
    "    where :math:`C` is the capacitance matrix :math:`I + W.T @ inv(D) @ W`, to compute the squared\n",
    "    Mahalanobis distance :math:`x.T @ inv(W @ W.T + D) @ x`.\n",
    "    \"\"\"\n",
    "    Wt_Dinv = W.mT / D.unsqueeze(-2)\n",
    "    Wt_Dinv_x = _batch_mv(Wt_Dinv, x)\n",
    "    mahalanobis_term1 = (x.pow(2) / D).sum(-1)\n",
    "    mahalanobis_term2 = _batch_mahalanobis(capacitance_tril, Wt_Dinv_x)\n",
    "    return mahalanobis_term1 - mahalanobis_term2\n",
    "\n",
    "class LowRankMultivariateStudentT_V3(TorchDistribution):\n",
    "    \"\"\"\n",
    "    Creates a multivariate t distribution with covariance matrix having a low-rank\n",
    "    form parameterized by :attr:`cov_factor` and :attr:`cov_diag`::\n",
    "        covariance_matrix = cov_factor @ cov_factor.T + cov_diag\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    df (Tensor): degrees of freedom of the distribution\n",
    "    loc (Tensor): mean of the distribution with shape `batch_shape + event_shape`\n",
    "    cov_factor (Tensor): factor part of low-rank form of covariance matrix with shape\n",
    "        `batch_shape + event_shape + (rank,)`\n",
    "    cov_diag (Tensor): diagonal part of low-rank form of covariance matrix with shape\n",
    "        `batch_shape + event_shape`\n",
    "\n",
    "    Note:\n",
    "        The computation for determinant and inverse of covariance matrix is avoided when\n",
    "        `cov_factor.shape[1] << cov_factor.shape[0]` thanks to `Woodbury matrix identity\n",
    "        <https://en.wikipedia.org/wiki/Woodbury_matrix_identity>`_ and\n",
    "        `matrix determinant lemma <https://en.wikipedia.org/wiki/Matrix_determinant_lemma>`_.\n",
    "        Thanks to these formulas, we just need to compute the determinant and inverse of\n",
    "        the small size \"capacitance\" matrix::\n",
    "\n",
    "            capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n",
    "    \"\"\"\n",
    "    \n",
    "    arg_constraints = {\"df\": constraints.positive,\n",
    "                       \"loc\": constraints.real_vector,\n",
    "                       \"cov_factor\": constraints.independent(constraints.real, 2),\n",
    "                       \"cov_diag\": constraints.independent(constraints.positive, 1)}\n",
    "    \n",
    "    support = constraints.real_vector\n",
    "    has_rsample = True\n",
    "    \n",
    "    def __init__(self, df, loc, cov_factor, cov_diag, validate_args=None):\n",
    "        if loc.dim() < 1:\n",
    "            raise ValueError(\"loc must be at least one-dimensional.\")\n",
    "        event_shape = loc.shape[-1:]\n",
    "        if cov_factor.dim() < 2:\n",
    "            raise ValueError(\"cov_factor must be at least two_dimensional\")\n",
    "        if cov_factor.shape[-2:-1] != event_shape:\n",
    "            raise ValueError(\"cov_factor must be a batch of matrices with shape {} x m\".format(event_shape[0]))\n",
    "        if cov_diag.shape[-1:] != event_shape:\n",
    "            raise ValueError(\"cov_diag must be a batch of vectors with shape {}\".format(event_shape))\n",
    "            \n",
    "        if not isinstance(df, torch.Tensor):\n",
    "            df = loc.new_tensor(df)\n",
    "            \n",
    "        loc_ = loc.unsqueeze(-1)\n",
    "        cov_diag_ = cov_diag.unsqueeze(-1)\n",
    "        try:\n",
    "            loc_, self.cov_factor, cov_diag_ = torch.broadcast_tensors(loc_, cov_factor, cov_diag_)\n",
    "        except RuntimeError as e:\n",
    "            raise ValueError(\"Incompatible batch shapes: loc {}, cov_factor {}, cov_diag {}\"\n",
    "                             .format(loc.shape, cov_factor.shape, cov_diag.shape)) from e\n",
    "        \n",
    "        self.loc = loc_[..., 0]\n",
    "        self.cov_diag = cov_diag_[..., 0]\n",
    "        batch_shape = self.loc.shape[:-1]\n",
    "        self.df = df.expand(batch_shape)\n",
    "\n",
    "        self._unbroadcasted_cov_factor = cov_factor\n",
    "        self._unbroadcasted_cov_diag = cov_diag\n",
    "        self._capacitance_tril = _batch_capacitance_tril(cov_factor, cov_diag)\n",
    "        \n",
    "        self._chi2 = Chi2(self.df)\n",
    "        \n",
    "        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n",
    "        \n",
    "    def expand(self, batch_shape, _instance=None):\n",
    "        new = self._get_checked_instance(LowRankMultivariateNormal, _instance)\n",
    "        batch_shape = torch.Size(batch_shape)\n",
    "        loc_shape = batch_shape + self.event_shape\n",
    "        new.df = self.df = df.expand(loc_shape)\n",
    "        new.loc = self.loc.expand(loc_shape)\n",
    "        new._chi2 = self._chi2.expand(loc_shape)\n",
    "        new.cov_diag = self.cov_diag.expand(loc_shape)\n",
    "        new.cov_factor = self.cov_factor.expand(loc_shape + self.cov_factor.shape[-1:])\n",
    "        new._unbroadcasted_cov_factor = self._unbroadcasted_cov_factor\n",
    "        new._unbroadcasted_cov_diag = self._unbroadcasted_cov_diag\n",
    "        new._capacitance_tril = self._capacitance_tril\n",
    "        super(LowRankMultivariateStudentT_V2, new).__init__(batch_shape,\n",
    "                                                       self.event_shape,\n",
    "                                                       validate_args=False)\n",
    "        new._validate_args = self._validate_args\n",
    "        return new\n",
    "    \n",
    "    @lazy_property\n",
    "    def scale_tril(self):\n",
    "        # The following identity is used to increase the numerically computation stability\n",
    "        # for Cholesky decomposition (see http://www.gaussianprocess.org/gpml/, Section 3.4.3):\n",
    "        #     W @ W.T + D = D1/2 @ (I + D-1/2 @ W @ W.T @ D-1/2) @ D1/2\n",
    "        # The matrix \"I + D-1/2 @ W @ W.T @ D-1/2\" has eigenvalues bounded from below by 1,\n",
    "        # hence it is well-conditioned and safe to take Cholesky decomposition.\n",
    "        n = self._event_shape[0]\n",
    "        cov_diag_sqrt_unsqueeze = self._unbroadcasted_cov_diag.sqrt().unsqueeze(-1)\n",
    "        Dinvsqrt_W = self._unbroadcasted_cov_factor / cov_diag_sqrt_unsqueeze\n",
    "        K = torch.matmul(Dinvsqrt_W, Dinvsqrt_W.mT).contiguous()\n",
    "        K.view(-1, n * n)[:, ::n + 1] += 1  # add identity matrix to K\n",
    "        scale_tril = cov_diag_sqrt_unsqueeze * torch.linalg.cholesky(K)\n",
    "        return scale_tril.expand(self._batch_shape + self._event_shape + self._event_shape)\n",
    "\n",
    "    @lazy_property\n",
    "    def covariance_matrix(self):\n",
    "        # NB: this is not covariance of this distribution;\n",
    "        # the actual covariance is df / (df - 2) * covariance_matrix\n",
    "        covariance_matrix = (torch.matmul(self._unbroadcasted_cov_factor,\n",
    "                                          self._unbroadcasted_cov_factor.mT)\n",
    "                             + torch.diag_embed(self._unbroadcasted_cov_diag))\n",
    "        return covariance_matrix.expand(self._batch_shape + self._event_shape +\n",
    "                                        self._event_shape)\n",
    "\n",
    "    @lazy_property\n",
    "    def precision_matrix(self):\n",
    "        # We use \"Woodbury matrix identity\" to take advantage of low rank form::\n",
    "        #     inv(W @ W.T + D) = inv(D) - inv(D) @ W @ inv(C) @ W.T @ inv(D)\n",
    "        # where :math:`C` is the capacitance matrix.\n",
    "        Wt_Dinv = (self._unbroadcasted_cov_factor.mT\n",
    "                   / self._unbroadcasted_cov_diag.unsqueeze(-2))\n",
    "        A = torch.linalg.solve_triangular(self._capacitance_tril, Wt_Dinv, upper=False)\n",
    "        precision_matrix = torch.diag_embed(self._unbroadcasted_cov_diag.reciprocal()) - A.mT @ A\n",
    "        return precision_matrix.expand(self._batch_shape + self._event_shape +\n",
    "                                       self._event_shape)\n",
    "    \n",
    "    def rsample(self, sample_shape=torch.Size()):\n",
    "        shape = self._extended_shape(sample_shape)\n",
    "        #X = torch.empty(shape, dtype=self.df.dtype, device=self.df.device).normal_()\n",
    "        Z = self._chi2.rsample(sample_shape)\n",
    "        #Y = X * torch.rsqrt(Z / self.df).unsqueeze(-1)\n",
    "        #return self.loc + self.scale_tril.matmul(Y.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        W_shape = shape[:-1] + self.cov_factor.shape[-1:]\n",
    "        eps_W = _standard_normal(W_shape, dtype=self.loc.dtype, device=self.loc.device)\n",
    "        eps_D = _standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)\n",
    "        \n",
    "        frac = torch.rsqrt(Z / self.df).unsqueeze(-1)\n",
    "        Yeps_W = eps_W * frac\n",
    "        Yeps_D = eps_D * frac\n",
    "        return (self.loc + _batch_mv(self._unbroadcasted_cov_factor, Yeps_W)\n",
    "                + self._unbroadcasted_cov_diag.sqrt() * Yeps_D)\n",
    "        \n",
    "\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        if self._validate_args:\n",
    "            self._validate_sample(value)\n",
    "        n = self.loc.size(-1)\n",
    "        diff = (value - self.loc)\n",
    "        y = _batch_lowrank_mahalanobis(self._unbroadcasted_cov_factor,\n",
    "                                       self._unbroadcasted_cov_diag,\n",
    "                                       diff,\n",
    "                                       self._capacitance_tril)\n",
    "        \n",
    "        log_det = _batch_lowrank_logdet(self._unbroadcasted_cov_factor,\n",
    "                                        self._unbroadcasted_cov_diag,\n",
    "                                        self._capacitance_tril)\n",
    "        Z = (\n",
    "            log_det * 0.5\n",
    "            + 0.5 * n * self.df.log()\n",
    "            + 0.5 * n * math.log(math.pi)\n",
    "            + torch.lgamma(0.5 * self.df)\n",
    "            - torch.lgamma(0.5 * (self.df + n))\n",
    "        )\n",
    "        return -0.5 * (self.df + n) * torch.log1p(y / self.df) - Z\n",
    "\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        m = self.loc.clone()\n",
    "        m[self.df <= 1, :] = float(\"nan\")\n",
    "        return m\n",
    "\n",
    "    @property\n",
    "    def variance(self):\n",
    "        m = self.scale_tril.pow(2).sum(-1) * (self.df / (self.df - 2)).unsqueeze(-1)\n",
    "        m[(self.df <= 2) & (self.df > 1), :] = float(\"inf\")\n",
    "        m[self.df <= 1, :] = float(\"nan\")\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13875f43-101c-4d6d-9254-47cb7a5c1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as transforms\n",
    "\n",
    "class HmResBlock(nn.Module):\n",
    "    def __init__(self, channels, p):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=7, stride=1, dilation=2, padding='same')\n",
    "        self.dropout1 = nn.Dropout2d(p)\n",
    "        self.norm1 = nn.InstanceNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, dilation=2, padding='same')\n",
    "        self.norm2 = nn.InstanceNorm2d(channels)\n",
    "        self.activ = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.activ(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.activ(out)\n",
    "        \n",
    "        out = out + identity\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, ins, outs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ins, outs, kernel_size=3, stride=2, dilation=1, padding=1)\n",
    "        self.norm = nn.InstanceNorm2d(outs)\n",
    "        self.activ = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activ(self.norm(self.conv(x)))\n",
    "\n",
    "class HmUpsampBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels*2, channels, kernel_size=3, stride=1, dilation=1, padding='same')\n",
    "        self.norm = nn.InstanceNorm2d(channels)\n",
    "        self.activ = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.interpolate(x, scale_factor=2, mode='bilinear')\n",
    "        return self.activ(self.norm(self.conv(out)))\n",
    "        \n",
    "\n",
    "class HmFeatureBlock(nn.Module):\n",
    "    def __init__(self, ins):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(ins, ins//2, kernel_size=3, stride=1, dilation=2, padding='same')\n",
    "        self.activ = nn.ReLU()\n",
    "        self.norm1 = nn.InstanceNorm2d(ins)\n",
    "        self.conv2 = nn.Conv2d(ins//2, ins//2, kernel_size=1, stride=1, dilation=1)\n",
    "        self.norm2 = nn.InstanceNorm2d(ins)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.activ(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.activ(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class HyperMapREDO(nn.Module):\n",
    "    def __init__(self,dropout_p = 0., encoder_sizes=[16,32,64,128,256], inchannels=3, outchannels=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        # input layer\n",
    "        self.conv_first = nn.Conv2d(inchannels, encoder_sizes[0], kernel_size=5, stride=1, dilation=1, padding='same')\n",
    "        self.activ = nn.ReLU()\n",
    "        \n",
    "        # encoder section\n",
    "        l = len(encoder_sizes) - 1\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            DownBlock(encoder_sizes[i], encoder_sizes[i+1]) for i in range(0, l)\n",
    "        ])\n",
    "        \n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            HmResBlock(c, dropout_p) for c in encoder_sizes\n",
    "        ])\n",
    "        \n",
    "        # decoder section\n",
    "        self.upsample_blocks = nn.ModuleList([\n",
    "            HmUpsampBlock(c) for c in encoder_sizes[:-1][::-1]\n",
    "        ])\n",
    "        \n",
    "        self.feature_blocks = nn.ModuleList([\n",
    "            HmFeatureBlock(encoder_sizes[l - i]) for i in range(l-1)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        # multi-scale feature section\n",
    "        self.ms_feature_layers = nn.ModuleList([\n",
    "            nn.Conv2d(encoder_sizes[2], encoder_sizes[1], 3, padding='same'),\n",
    "            nn.Conv2d(encoder_sizes[1], encoder_sizes[1], 3, padding='same'),\n",
    "            nn.Conv2d(encoder_sizes[1], encoder_sizes[1], 3, padding='same')\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        # output layer\n",
    "        self.last_1 = nn.Conv2d(encoder_sizes[1], encoder_sizes[1], 3, padding='same')\n",
    "        self.last_2 = nn.Conv2d(encoder_sizes[1]*3, encoder_sizes[1], 1)\n",
    "        self.last_3 = nn.Conv2d(encoder_sizes[1], outchannels, 1)\n",
    "        self.last_norm = nn.InstanceNorm2d(encoder_sizes[1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input layer\n",
    "        out = self.activ(self.conv_first(x))\n",
    "        # print(out.shape)\n",
    "        \n",
    "        skips = []\n",
    "        \n",
    "        # encoder section\n",
    "        out = self.res_blocks[0](out)\n",
    "        # print(out.shape)\n",
    "        skips.append(out)\n",
    "        for i in range(len(self.res_blocks) - 1):\n",
    "            out = self.down_blocks[i](out)\n",
    "            out = self.res_blocks[i+1](out)\n",
    "            # print(\"loop: \", out.shape)\n",
    "            skips.append(out)\n",
    "        \n",
    "        # decoder section\n",
    "        ml_features = []\n",
    "        out = skips.pop()\n",
    "        for i in range(len(self.upsample_blocks)):\n",
    "            # print(\"dec\")\n",
    "            if i > 0:\n",
    "                sk = skips.pop()\n",
    "                sk = transforms.center_crop(sk, out.shape[-2:])\n",
    "                out = torch.cat([out, sk], dim=1)\n",
    "                out = self.feature_blocks[i-1](out)\n",
    "            \n",
    "            if i > 1:\n",
    "                ml_features.append(self.ms_feature_layers[i-2](out))\n",
    "                \n",
    "            out = self.upsample_blocks[i](out)\n",
    "        \n",
    "        # final layers\n",
    "        sk = skips.pop()\n",
    "        sk = transforms.center_crop(sk, out.shape[-2:])\n",
    "        out = torch.cat([out, sk], dim=1)\n",
    "        out = self.last_norm(self.activ(self.last_1(out)))\n",
    "        \n",
    "        # multiscale feature section\n",
    "        ml_features = [out] + ml_features\n",
    "        ml_features = [F.interpolate(mf, size=x.shape[-2:], mode='bilinear') for mf in ml_features]\n",
    "        combined_features = torch.cat(ml_features, dim=1)\n",
    "        \n",
    "        out = self.activ(self.last_2(combined_features))\n",
    "        out = self.last_3(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cb08829-1f8b-4bad-a376-6982fa471bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowRankMultivariateNormalCustom(td.Distribution):\n",
    "    r\"\"\"\n",
    "    Creates a multivariate normal distribution with covariance matrix having a low-rank form\n",
    "    parameterized by :attr:`cov_factor` and :attr:`cov_diag`::\n",
    "\n",
    "        covariance_matrix = cov_factor @ cov_factor.T + cov_diag\n",
    "\n",
    "    Example:\n",
    "        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_LAPACK)\n",
    "        >>> # xdoctest: +IGNORE_WANT(\"non-determenistic\")\n",
    "        >>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\n",
    "        >>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\n",
    "        tensor([-0.2102, -0.5429])\n",
    "\n",
    "    Args:\n",
    "        loc (Tensor): mean of the distribution with shape `batch_shape + event_shape`\n",
    "        cov_factor (Tensor): factor part of low-rank form of covariance matrix with shape\n",
    "            `batch_shape + event_shape + (rank,)`\n",
    "        cov_diag (Tensor): diagonal part of low-rank form of covariance matrix with shape\n",
    "            `batch_shape + event_shape`\n",
    "\n",
    "    Note:\n",
    "        The computation for determinant and inverse of covariance matrix is avoided when\n",
    "        `cov_factor.shape[1] << cov_factor.shape[0]` thanks to `Woodbury matrix identity\n",
    "        <https://en.wikipedia.org/wiki/Woodbury_matrix_identity>`_ and\n",
    "        `matrix determinant lemma <https://en.wikipedia.org/wiki/Matrix_determinant_lemma>`_.\n",
    "        Thanks to these formulas, we just need to compute the determinant and inverse of\n",
    "        the small size \"capacitance\" matrix::\n",
    "\n",
    "            capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n",
    "    \"\"\"\n",
    "    arg_constraints = {\"loc\": constraints.real_vector,\n",
    "                       \"cov_factor\": constraints.independent(constraints.real, 2),\n",
    "                       \"cov_diag\": constraints.independent(constraints.positive, 1)}\n",
    "    support = constraints.real_vector\n",
    "    has_rsample = True\n",
    "\n",
    "    def __init__(self, loc, cov_factor, cov_diag, validate_args=None):\n",
    "        if loc.dim() < 1:\n",
    "            raise ValueError(\"loc must be at least one-dimensional.\")\n",
    "        event_shape = loc.shape[-1:]\n",
    "        if cov_factor.dim() < 2:\n",
    "            raise ValueError(\"cov_factor must be at least two-dimensional, \"\n",
    "                             \"with optional leading batch dimensions\")\n",
    "        if cov_factor.shape[-2:-1] != event_shape:\n",
    "            raise ValueError(\"cov_factor must be a batch of matrices with shape {} x m\"\n",
    "                             .format(event_shape[0]))\n",
    "        if cov_diag.shape[-1:] != event_shape:\n",
    "            raise ValueError(\"cov_diag must be a batch of vectors with shape {}\".format(event_shape))\n",
    "\n",
    "        loc_ = loc.unsqueeze(-1)\n",
    "        cov_diag_ = cov_diag.unsqueeze(-1)\n",
    "        try:\n",
    "            loc_, self.cov_factor, cov_diag_ = torch.broadcast_tensors(loc_, cov_factor, cov_diag_)\n",
    "        except RuntimeError as e:\n",
    "            raise ValueError(\"Incompatible batch shapes: loc {}, cov_factor {}, cov_diag {}\"\n",
    "                             .format(loc.shape, cov_factor.shape, cov_diag.shape)) from e\n",
    "        self.loc = loc_[..., 0]\n",
    "        self.cov_diag = cov_diag_[..., 0]\n",
    "        batch_shape = self.loc.shape[:-1]\n",
    "\n",
    "        self._unbroadcasted_cov_factor = cov_factor\n",
    "        self._unbroadcasted_cov_diag = cov_diag\n",
    "        #self._capacitance_tril = _batch_capacitance_tril(cov_factor, cov_diag)\n",
    "        super().__init__(batch_shape, event_shape,\n",
    "                                                        validate_args=validate_args)\n",
    "\n",
    "    def expand(self, batch_shape, _instance=None):\n",
    "        new = self._get_checked_instance(LowRankMultivariateNormal, _instance)\n",
    "        batch_shape = torch.Size(batch_shape)\n",
    "        loc_shape = batch_shape + self.event_shape\n",
    "        new.loc = self.loc.expand(loc_shape)\n",
    "        new.cov_diag = self.cov_diag.expand(loc_shape)\n",
    "        new.cov_factor = self.cov_factor.expand(loc_shape + self.cov_factor.shape[-1:])\n",
    "        new._unbroadcasted_cov_factor = self._unbroadcasted_cov_factor\n",
    "        new._unbroadcasted_cov_diag = self._unbroadcasted_cov_diag\n",
    "        new._capacitance_tril = self._capacitance_tril\n",
    "        super(LowRankMultivariateNormal, new).__init__(batch_shape,\n",
    "                                                       self.event_shape,\n",
    "                                                       validate_args=False)\n",
    "        new._validate_args = self._validate_args\n",
    "        return new\n",
    "\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.loc\n",
    "\n",
    "    @property\n",
    "    def mode(self):\n",
    "        return self.loc\n",
    "\n",
    "    @lazy_property\n",
    "    def variance(self):\n",
    "        return (self._unbroadcasted_cov_factor.pow(2).sum(-1)\n",
    "                + self._unbroadcasted_cov_diag).expand(self._batch_shape + self._event_shape)\n",
    "\n",
    "    @lazy_property\n",
    "    def scale_tril(self):\n",
    "        # The following identity is used to increase the numerically computation stability\n",
    "        # for Cholesky decomposition (see http://www.gaussianprocess.org/gpml/, Section 3.4.3):\n",
    "        #     W @ W.T + D = D1/2 @ (I + D-1/2 @ W @ W.T @ D-1/2) @ D1/2\n",
    "        # The matrix \"I + D-1/2 @ W @ W.T @ D-1/2\" has eigenvalues bounded from below by 1,\n",
    "        # hence it is well-conditioned and safe to take Cholesky decomposition.\n",
    "        n = self._event_shape[0]\n",
    "        cov_diag_sqrt_unsqueeze = self._unbroadcasted_cov_diag.sqrt().unsqueeze(-1)\n",
    "        Dinvsqrt_W = self._unbroadcasted_cov_factor / cov_diag_sqrt_unsqueeze\n",
    "        K = torch.matmul(Dinvsqrt_W, Dinvsqrt_W.mT).contiguous()\n",
    "        K.view(-1, n * n)[:, ::n + 1] += 1  # add identity matrix to K\n",
    "        scale_tril = cov_diag_sqrt_unsqueeze * torch.linalg.cholesky(K)\n",
    "        return scale_tril.expand(self._batch_shape + self._event_shape + self._event_shape)\n",
    "\n",
    "    @lazy_property\n",
    "    def covariance_matrix(self):\n",
    "        covariance_matrix = (torch.matmul(self._unbroadcasted_cov_factor,\n",
    "                                          self._unbroadcasted_cov_factor.mT)\n",
    "                             + torch.diag_embed(self._unbroadcasted_cov_diag))\n",
    "        return covariance_matrix.expand(self._batch_shape + self._event_shape +\n",
    "                                        self._event_shape)\n",
    "\n",
    "    @lazy_property\n",
    "    def precision_matrix(self):\n",
    "        # We use \"Woodbury matrix identity\" to take advantage of low rank form::\n",
    "        #     inv(W @ W.T + D) = inv(D) - inv(D) @ W @ inv(C) @ W.T @ inv(D)\n",
    "        # where :math:`C` is the capacitance matrix.\n",
    "        Wt_Dinv = (self._unbroadcasted_cov_factor.mT\n",
    "                   / self._unbroadcasted_cov_diag.unsqueeze(-2))\n",
    "        A = torch.linalg.solve_triangular(self._capacitance_tril, Wt_Dinv, upper=False)\n",
    "        precision_matrix = torch.diag_embed(self._unbroadcasted_cov_diag.reciprocal()) - A.mT @ A\n",
    "        return precision_matrix.expand(self._batch_shape + self._event_shape +\n",
    "                                       self._event_shape)\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size()):\n",
    "        shape = self._extended_shape(sample_shape)\n",
    "        W_shape = shape[:-1] + self.cov_factor.shape[-1:]\n",
    "        eps_W = _standard_normal(W_shape, dtype=self.loc.dtype, device=self.loc.device)\n",
    "        eps_D = _standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)\n",
    "        return (self.loc + _batch_mv(self._unbroadcasted_cov_factor, eps_W)\n",
    "                + self._unbroadcasted_cov_diag.sqrt() * eps_D)\n",
    "\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        if self._validate_args:\n",
    "            self._validate_sample(value)\n",
    "        diff = value - self.loc\n",
    "        M = _batch_lowrank_mahalanobis(self._unbroadcasted_cov_factor,\n",
    "                                       self._unbroadcasted_cov_diag,\n",
    "                                       diff,\n",
    "                                       self._capacitance_tril)\n",
    "        log_det = _batch_lowrank_logdet(self._unbroadcasted_cov_factor,\n",
    "                                        self._unbroadcasted_cov_diag,\n",
    "                                        self._capacitance_tril)\n",
    "        return -0.5 * (self._event_shape[0] * math.log(2 * math.pi) + log_det + M)\n",
    "\n",
    "\n",
    "    def entropy(self):\n",
    "        log_det = _batch_lowrank_logdet(self._unbroadcasted_cov_factor,\n",
    "                                        self._unbroadcasted_cov_diag,\n",
    "                                        self._capacitance_tril)\n",
    "        H = 0.5 * (self._event_shape[0] * (1.0 + math.log(2 * math.pi)) + log_det)\n",
    "        if len(self._batch_shape) == 0:\n",
    "            return H\n",
    "        else:\n",
    "            return H.expand(self._batch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9c067ec-471a-49c0-8735-0b1650414882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperMapp3rSSN2(HyperMapREDO):\n",
    "    def __init__(self,\n",
    "                 dropout_p = 0., encoder_sizes=[16,32,64,128,256], inchannels=3, out_channels=2,\n",
    "                 ssn_rank = 10,\n",
    "                 ssn_epsilon=1e-5,\n",
    "                 ssn_diagonal=False,\n",
    "                 dims=2\n",
    "                ):\n",
    "        super().__init__(dropout_p, encoder_sizes, inchannels, outchannels=encoder_sizes[0]) # last layer of just keeps number of nodes fixed this time)\n",
    "        \n",
    "        print(\"WARNING: this model assumes that the input to the model contains the brain mask in the first channel!\")\n",
    "        conv_func = get_conv_func(dims, transpose=False)\n",
    "        self.ssn_rank = ssn_rank\n",
    "        self.ssn_diagonal = ssn_diagonal\n",
    "        self.ssn_epsilon = ssn_epsilon\n",
    "        self.ssn_num_classes = out_channels\n",
    "        \n",
    "        self.lrelu = nn.LeakyReLU(0.01)\n",
    "        \n",
    "        self.mean_l = conv_func(encoder_sizes[0], out_channels, kernel_size = (1,) *  dims, padding='same')\n",
    "        self.log_cov_diag_l = conv_func(encoder_sizes[0], out_channels, kernel_size = (1,) * dims, padding='same')\n",
    "        self.cov_factor_l = conv_func(encoder_sizes[0], out_channels * ssn_rank, kernel_size = (1,) * dims, padding='same')\n",
    "        #self.vk_l = conv_func(encoder_sizes[0], 2, kernel_size=7, padding='same')\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.lrelu(super().forward(x))\n",
    "\n",
    "        if torch.sum(torch.isnan(logits)) > 0:\n",
    "            print(\"NAN 1\", torch.sum(torch.isnan(logits)))\n",
    "        batch_size = logits.shape[0]\n",
    "        event_shape = (self.ssn_num_classes,) + logits.shape[2:]\n",
    "        \n",
    "        mean = self.mean_l(logits)\n",
    "        mean = mean.view((batch_size, -1))\n",
    "        \n",
    "        cov_diag = self.log_cov_diag_l(logits).exp() + self.ssn_epsilon\n",
    "        cov_diag = cov_diag.view((batch_size, -1))\n",
    "        \n",
    "        cov_factor = self.cov_factor_l(logits)\n",
    "        cov_factor = cov_factor.view((batch_size, self.ssn_rank, self.ssn_num_classes, -1))\n",
    "        cov_factor = cov_factor.flatten(2,3)\n",
    "        cov_factor = cov_factor.transpose(1,2)\n",
    "        if torch.sum(torch.isnan(mean)) > 0:\n",
    "            print(\"NAN 2\")\n",
    "        if torch.sum(torch.isnan(cov_diag)) > 0:\n",
    "            print(\"NAN 3\")\n",
    "        if torch.sum(torch.isnan(cov_factor)) > 0:\n",
    "            print(\"NAN 4\")\n",
    "            \n",
    "            \n",
    "        #vk = self.vk_l(logits).exp()\n",
    "        # print(vk.shape)\n",
    "        #vk = vk.mean(dim=(-1, -2)) # mean along each axis except for channel, yielding two values\n",
    "        #D = mean.shape[1]\n",
    "        #v = vk[:,0]\n",
    "        #k = vk[:,1]\n",
    "        #evidence_scale = (k+1) / (k*v)\n",
    "        \n",
    "        # print(\"vk shapes\")\n",
    "        # print(v.shape)\n",
    "        # print(k.shape)\n",
    "        # print(v, k, evidence_scale)\n",
    "        \n",
    "        # covariance tends to blow up to infinity, hence set to 0 outside the ROI\n",
    "        mask = x[:,1]\n",
    "        mask = mask.unsqueeze(1).expand((batch_size, self.ssn_num_classes) + mask.shape[1:]).reshape(batch_size, -1)\n",
    "        cov_factor = cov_factor * mask.unsqueeze(-1)\n",
    "        cov_diag = cov_diag * mask + self.ssn_epsilon\n",
    "        \n",
    "        if torch.sum(torch.isnan(cov_diag)) > 0:\n",
    "            print(\"NAN 3\")\n",
    "        if torch.sum(torch.isnan(cov_factor)) > 0:\n",
    "            print(\"NAN 4\")\n",
    "        \n",
    "        # print(evidence_scale.shape, (evidence_scale**0.5).shape)\n",
    "        # print(cov_diag.shape, cov_factor.shape)\n",
    "#         cov_diag *= evidence_scale.unsqueeze(-1)\n",
    "#         cov_factor *= (evidence_scale**0.5).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        if torch.sum(torch.isnan(mask)) > 0:\n",
    "            print(\"NAN 5\")\n",
    "        if torch.sum(torch.isnan(cov_factor)) > 0:\n",
    "            print(\"NAN 6\")\n",
    "        if torch.sum(torch.isnan(cov_diag)) > 0:\n",
    "            print(\"NAN 7\")\n",
    "            \n",
    "        # print(cov_diag)\n",
    "        \n",
    "        if self.ssn_diagonal:\n",
    "            base_distribution = td.Independent(td.Normal(loc=mean, scale=torch.sqrt(cov_diag)), 1)\n",
    "        else:\n",
    "            try:\n",
    "                base_distribution = LowRankMultivariateNormalCustom(loc=mean, cov_factor=cov_factor, cov_diag=cov_diag)\n",
    "                #base_distribution = LowRankMultivariateStudentT_V3(df=v, loc=mean, cov_factor=cov_factor, cov_diag=cov_diag)\n",
    "                #print(\"using multivariate normal!\")\n",
    "            except Exception as e:\n",
    "                print(\"was thrown: \", e)\n",
    "                print('hmm: Covariance became non invertible using independent normals for this batch!')\n",
    "                print(\"cov diag okay: \", torch.sum(cov_diag <=0))\n",
    "                print(\"sqrt cov diag okay: \", torch.sum(torch.sqrt(cov_diag) <=0))\n",
    "                \n",
    "                try:\n",
    "                    base_distribution = td.Independent(td.Normal(loc=mean, scale=torch.sqrt(cov_diag)),1)\n",
    "                except Exception as e:\n",
    "                    print(\"second fail: \", e)\n",
    "                    print(torch.min(torch.sqrt(cov_diag), torch.max(torch.sqrt(cov_diag))))\n",
    "        \n",
    "        distribution = ReshapedDistribution(base_distribution, event_shape)\n",
    "        \n",
    "        shape = (batch_size,) + event_shape\n",
    "        logit_mean_view = mean.view(shape).detach()\n",
    "        cov_diag_view = cov_diag.view(shape).detach()\n",
    "        cov_factor_view = cov_factor.transpose(2,1).view((batch_size, self.ssn_num_classes * self.ssn_rank) + event_shape[1:]).detach()\n",
    "        \n",
    "        # compute the diagonal of the precision matrix for the evidence regularizer\n",
    "#         U = cov_factor\n",
    "#         D_inv = 1./cov_diag\n",
    "#         # print(\"shapes for regularizer\")\n",
    "#         # print(\"U, U.mt\", U.shape, U.mT.shape)\n",
    "#         # print(\"D\", D_inv.shape)\n",
    "        \n",
    "#         D_inv_mult = D_inv.unsqueeze(-1).expand(U.shape)\n",
    "#         # print(\"D inv mult\", D_inv_mult.shape)\n",
    "        \n",
    "#         F = torch.eye(self.ssn_rank).to(U.device) + U.mT.bmm(D_inv_mult * U)\n",
    "#         # print(\"F\", F.shape)\n",
    "        \n",
    "#         RRT = torch.cholesky_inverse(F)\n",
    "#         R = torch.cholesky(RRT)\n",
    "        \n",
    "#         # print(\"R\", R.shape)\n",
    "#         V = (D_inv_mult * U).bmm(R)\n",
    "#         # print(\"V\", V.shape)\n",
    "        \n",
    "#         # print(\"diag v\", torch.diagonal(V, dim1=1, dim2=2).shape, V.shape)\n",
    "        \n",
    "#         pres_diag = D_inv - torch.sum(V * V, dim=2) # get the diagonal of the V@V.T matrix without computing it\n",
    "             \n",
    "        \n",
    "        output_dict = {\n",
    "            # 'v':v,\n",
    "            # 'k':k,\n",
    "            'logit_mean':logit_mean_view,\n",
    "            'cov_diag':cov_diag_view,\n",
    "            'cov_factor':cov_factor_view,\n",
    "            'distribution':distribution,\n",
    "            # 'pres_diag':pres_diag,\n",
    "        }\n",
    "        \n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65e68de3-0454-44aa-893a-255cd3a1e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_raw = HyperMapp3rSSN2().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47444f31-f60a-406a-b98c-1f20b10fc8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = StandardLitModelWrapper.load_from_checkpoint('/disk/scratch_big/s2208943/results/new_tests/epoch=15-step=2192.ckpt', model=model_raw, loss=loss, \n",
    "#                                 logging_metric=SsnDiceMetricWrapper,\n",
    "#                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a81ef93-d78b-4638-8b61-d82baa47e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_re_parametrization_trick(dist, num_samples):\n",
    "    assert num_samples % 2 == 0\n",
    "    samples = dist.rsample((num_samples // 2,))\n",
    "    mean = dist.mean.unsqueeze(0)\n",
    "    samples = samples - mean\n",
    "    return torch.cat([samples, -samples]) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "287a10f1-6cd1-4d91-bab6-2b22cce8ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist = model_raw(x.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0314ceed-3a03-4b2f-8b02-f23a686bef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l = loss(dist, y.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "464a2317-bad0-4f61-bf59-6de11b9fcfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss(dist, y.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42c9dc6b-382b-4a81-bc8e-71202a273ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_loss(dist, target):\n",
    "    y = target\n",
    "    m = dist['logit_mean']\n",
    "    v = torch.nn.functional.softmax(m, dim=1)\n",
    "    \n",
    "    shape = [*y.unsqueeze(1).shape]\n",
    "    shape[1] = 2 # 2 classes\n",
    "    bs = shape[0]\n",
    "    \n",
    "    a = torch.zeros(shape)\n",
    "    \n",
    "    a[:,1] = y\n",
    "    a[:,0] = 1-y\n",
    "    pair_y = a.to(v.device)\n",
    "    \n",
    "    diff = (v.view(bs, -1) - pair_y.reshape(bs, -1)).abs().view(bs, -1) * 0.5\n",
    "    \n",
    "    correction = dist['v'].view(-1, 1).expand(diff.shape) + dist['k'].view(-1, 1).expand(diff.shape) * dist['pres_diag']\n",
    "    \n",
    "    closs = (correction * diff).mean()\n",
    "    \n",
    "    closs = closs.clip(0,1000)\n",
    "    \n",
    "    return closs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c2d279d-1657-4697-b570-64f88a189c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticSegmentationNetworkLossMCIntegral(nn.Module):\n",
    "    def __init__(self, num_mc_samples: int = 1):\n",
    "        super().__init__()\n",
    "        self.num_mc_samples = num_mc_samples\n",
    "\n",
    "    @staticmethod\n",
    "    def fixed_re_parametrization_trick(dist, num_samples):\n",
    "        assert num_samples % 2 == 0\n",
    "        samples = dist.rsample((num_samples // 2,))\n",
    "        mean = dist.mean.unsqueeze(0)\n",
    "        samples = samples - mean\n",
    "        return torch.cat([samples, -samples]) + mean\n",
    "\n",
    "    def forward(self, result_dict, target, **kwargs):\n",
    "        logits = result_dict['logit_mean']\n",
    "        distribution = result_dict['distribution']\n",
    "        \n",
    "        batch_size = logits.shape[0]\n",
    "        num_classes = logits.shape[1]\n",
    "        assert num_classes >= 2  # not implemented for binary case with implied background\n",
    "        # logit_sample = distribution.rsample((self.num_mc_samples,))\n",
    "        logit_sample = self.fixed_re_parametrization_trick(distribution, self.num_mc_samples)\n",
    "        target = target.unsqueeze(1)\n",
    "        target = target.expand((self.num_mc_samples,) + target.shape)\n",
    "\n",
    "        flat_size = self.num_mc_samples * batch_size\n",
    "        logit_sample = logit_sample.view((flat_size, num_classes, -1))\n",
    "        target = target.reshape((flat_size, -1))\n",
    "\n",
    "        log_prob = -F.cross_entropy(logit_sample, target, reduction='none').view((self.num_mc_samples, batch_size, -1))\n",
    "        loglikelihood = torch.mean(torch.logsumexp(torch.sum(log_prob, dim=-1), dim=0) - math.log(self.num_mc_samples))\n",
    "        loss = -loglikelihood\n",
    "        return loss\n",
    "    \n",
    "def fixed_re_parametrization_trick(dist, num_samples):\n",
    "        assert num_samples % 2 == 0\n",
    "        samples = dist.rsample((num_samples // 2,))\n",
    "        mean = dist.mean.unsqueeze(0)\n",
    "        samples = samples - mean\n",
    "        return torch.cat([samples, -samples]) + mean\n",
    "\n",
    "\n",
    "class SsnNetworkMeanLossWrapper(nn.Module):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__()\n",
    "        self.loss = loss_func\n",
    "    def forward(self, result_dict, target):\n",
    "        mean = result_dict['logit_mean']\n",
    "        return self.loss(mean, target)\n",
    "    \n",
    "class SsnNetworkSampleLossWrapper(nn.Module):\n",
    "    def __init__(self, loss_func, samples=10):\n",
    "        super().__init__()\n",
    "        self.loss = loss_func\n",
    "        self.samples = samples\n",
    "    def forward(self, result_dict, target):\n",
    "        samples = fixed_re_parametrization_trick(result_dict['distribution'], self.samples).to(target.device)\n",
    "        loss = 0\n",
    "        for s in samples:\n",
    "            loss += self.loss(s, target)\n",
    "        return loss / self.samples\n",
    "    \n",
    "def brier(logits, target):\n",
    "    preds = torch.nn.functional.softmax(logits, dim=1)[:,1]\n",
    "    bs = preds.shape[0]\n",
    "    preds = preds.view(bs, -1)\n",
    "    target = target.view(bs, -1)\n",
    "\n",
    "    return torch.nn.functional.mse_loss(preds, target.type(torch.float32), reduction='sum')\n",
    "    \n",
    "    \n",
    "class SsnNetworkMuBrierLossWrapper(nn.Module):\n",
    "    def __init__(self, loss_func, samples=10):\n",
    "        super().__init__()\n",
    "        self.loss = loss_func\n",
    "        self.samples = samples\n",
    "    def forward(self, result_dict, target):\n",
    "        s = result_dict['distribution'].mean # samples[0]\n",
    "        #print(s.shape, result_dict['distribution'].mean.shape)\n",
    "        dice = self.loss(s, target)\n",
    "        samples = fixed_re_parametrization_trick(result_dict['distribution'], self.samples).to(target.device)\n",
    "        avd_loss = 0\n",
    "        for s in samples:\n",
    "            avd_loss += brier(s, target)\n",
    "        \n",
    "        return dice, avd_loss / self.samples\n",
    "    \n",
    "class SsnDiceMetricWrapper(DiceLossMetric):\n",
    "\n",
    "    def update(self, preds_dict, target: torch.Tensor):\n",
    "        super().update(preds_dict['logit_mean'], target)\n",
    "\n",
    "    def compute(self):\n",
    "        return super().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ebe3449-7758-4791-819d-c77cf30b0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssn_diceloss = SsnNetworkMuBrierLossWrapper(dice_loss)# SsnNetworkMeanLossWrapper(dice_loss)\n",
    "mc_loss = StochasticSegmentationNetworkLossMCIntegral(num_mc_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41481167-7b64-4b35-b1dc-5b67c7f3608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardLitModelWrapper(pl.LightningModule):\n",
    "        def __init__(self, model, loss=F.cross_entropy, logging_metric=None, optimizer_params={\"lr\":1e-3}, lr_scheduler_params={\"step_size\":30, \"gamma\":0.1}, is_uq_model=False,\n",
    "                    optimizer_constructor=None, lr_scheduler_constructor=None):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "            self.loss = loss\n",
    "            self.logging_metric_train = logging_metric()\n",
    "            self.logging_metric_val = logging_metric()\n",
    "            self.optim_params = optimizer_params\n",
    "            self.lr_scheduler_params = lr_scheduler_params\n",
    "            self.is_uq_model = False\n",
    "            self.optimizer_constructor = optimizer_constructor\n",
    "            self.lr_scheduler_constructor = lr_scheduler_constructor\n",
    "\n",
    "\n",
    "        def forward(self, x, **kwargs):\n",
    "            return self.model(x, **kwargs)\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            # optimizer and schedulers go in the configure optimizers hook\n",
    "            if self.optimizer_constructor:\n",
    "                optimizer = self.optimizer_constructor(self.parameters(), **self.optim_params)\n",
    "            else:\n",
    "                optimizer = torch.optim.Adam(self.parameters(), **self.optim_params)\n",
    "\n",
    "            if self.lr_scheduler_constructor:\n",
    "                lr_scheduler = self.lr_scheduler_constructor(optimizer, **self.lr_scheduler_params)\n",
    "            else:\n",
    "                lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, **self.lr_scheduler_params)\n",
    "\n",
    "            return [optimizer], [lr_scheduler]\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            \"\"\"\n",
    "            lightning automates the training loop, \n",
    "            does epoch, back_tracking, optimizers and schedulers,\n",
    "            and metric reduction.\n",
    "            we just define how we want to process a single batch. \n",
    "            we can optionally pass optimizer_idx if we want to define multiple optimizers within the configure_optimizers\n",
    "            hook, and I presume we can add our own parameters also to functions?\n",
    "            \"\"\"\n",
    "\n",
    "            if self.is_uq_model:\n",
    "                self.model.set_applyfunc(True)\n",
    "\n",
    "            X, y = batch\n",
    "            y_hat = self(X)\n",
    "            loss = self.loss(y_hat, y)\n",
    "\n",
    "            # metrics \n",
    "            if self.logging_metric_train:\n",
    "                self.logging_metric_train(y_hat, y)\n",
    "                self.log(f\"train_metric\", self.logging_metric_train, on_step=True, on_epoch=False, prog_bar=True)\n",
    "            self.log(\"train_loss\", loss)\n",
    "\n",
    "            return loss\n",
    "\n",
    "    #     def training_epoch_end(self, outs):\n",
    "    #         self.log('train_metric_epoch', self.logging_metric_train.compute())\n",
    "\n",
    "    #     def validation_epoch_end(self, outs):\n",
    "    #         self.log('val_metric_epoch', self.logging_metric_val.compute())\n",
    "\n",
    "        def validation_step(self, batch, batch_idx):\n",
    "            \"\"\"\n",
    "            note: call trainer.validate() automatically loads the best checkpoint if checkpointing was enabled during fitting\n",
    "            well yes I want to enable checkpointing but will deal with that later.\n",
    "            also it does stuff like model.eval() and torch.no_grad() automatically which is nice.\n",
    "            I will need a custom eval thing to do my dropout estimation but can solve that later too.\n",
    "            \"\"\"\n",
    "            if self.is_uq_model:\n",
    "                self.model.set_applyfunc(False)\n",
    "\n",
    "            X, y = batch\n",
    "            y_hat = self(X)\n",
    "            val_loss = self.loss(y_hat, y)\n",
    "\n",
    "            if self.logging_metric_val:\n",
    "                self.logging_metric_val(y_hat, y)\n",
    "                self.log(f\"val_metric\", self.logging_metric_val, on_step=True, on_epoch=True, prog_bar=True)\n",
    "            self.log(\"val_loss\", val_loss)\n",
    "\n",
    "        def test_step(self, batch, batch_idx):\n",
    "            \"\"\"\n",
    "            we would need to directly call this function using the trainer\n",
    "            \"\"\"\n",
    "\n",
    "            if self.is_uq_model:\n",
    "                self.model.set_applyfunc(False)\n",
    "\n",
    "            X, y = batch\n",
    "            y_hat = self(X)\n",
    "            test_loss = self.loss(y_hat, y)\n",
    "            self.log(\"test_loss\", test_loss)\n",
    "\n",
    "        def predict_step(self, batch, batch_idx):\n",
    "            \"\"\"\n",
    "            just for making predictions as opposed to collecting metrics etc\n",
    "            note to use this, we just call .predict(dataloader) and it then automates the look\n",
    "            these functions are for a single batch. Nice.\n",
    "            \"\"\"\n",
    "            X, y = batch\n",
    "            pred = self(X)\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce3bf63c-6159-462a-bbba-2f562a8e7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_factor = 5\n",
    "avd_factor = 0.0001\n",
    "    \n",
    "def double_loss(outs, target):\n",
    "    dice, avd = ssn_diceloss(outs, target)\n",
    "    return dice * dice_factor + mc_loss(outs, target) * 0.01  + avd_factor * avd\n",
    "\n",
    "def triple_loss(outs, target):\n",
    "    main_loss = ssn_diceloss(outs, target) * dice_factor + mc_loss(outs, target) * 0.01\n",
    "    \n",
    "    if main_loss < 60:\n",
    "        main_loss += correction_loss(outs, target) * 0.1\n",
    "    return main_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "542f4859-551e-4124-afd8-20b0e3a0a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = double_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8fb50c3-f3e3-452e-90ea-80f10f28c28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: this model assumes that the input to the model contains the brain mask in the first channel!\n"
     ]
    }
   ],
   "source": [
    "model_raw = HyperMapp3rSSN2(ssn_rank=15).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ea2ce0a-de52-4770-92f7-877ae360ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = double_loss\n",
    "#loss = triple_loss\n",
    "\n",
    "optimizer_params={\"lr\":2e-4}\n",
    "optimizer = torch.optim.Adam\n",
    "lr_scheduler_params={\"milestones\":[1000], \"gamma\":0.5}\n",
    "lr_scheduler_constructor = torch.optim.lr_scheduler.MultiStepLR\n",
    "\n",
    "trained_ckpt_dir = '/disk/scratch/s2208943/results/new_tests/evid_ssn/'\n",
    "trained_model = 'epoch=29-step=4110.ckpt'\n",
    "\n",
    "model = StandardLitModelWrapper(model_raw, loss, \n",
    "                                logging_metric=SsnDiceMetricWrapper, # lambda : None,\n",
    "                                optimizer_params=optimizer_params,\n",
    "                                lr_scheduler_params=lr_scheduler_params,\n",
    "                                is_uq_model=False,\n",
    "                                optimizer_constructor=optimizer,\n",
    "                                lr_scheduler_constructor=lr_scheduler_constructor\n",
    "                               )\n",
    "\n",
    "# model = StandardLitModelWrapper.load_from_checkpoint(trained_ckpt_dir + trained_model,\n",
    "#                                                      model_raw, loss, \n",
    "#                                 logging_metric=SsnDiceMetricWrapper, # lambda : None,\n",
    "#                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b91e26e5-55e9-485b-ad01-1c6c8dff58b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type                 | Params\n",
      "--------------------------------------------------------------\n",
      "0 | model                | HyperMapp3rSSN2      | 6.3 M \n",
      "1 | logging_metric_train | SsnDiceMetricWrapper | 0     \n",
      "2 | logging_metric_val   | SsnDiceMetricWrapper | 0     \n",
      "--------------------------------------------------------------\n",
      "6.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.3 M     Total params\n",
      "25.236    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33037c104bf347a6aebc3f9bc4412dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 51.674\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 28.300 >= min_delta = 0.01. New best score: 23.374\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 12.336 >= min_delta = 0.01. New best score: 11.037\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.704 >= min_delta = 0.01. New best score: 9.333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.076 >= min_delta = 0.01. New best score: 8.257\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.463 >= min_delta = 0.01. New best score: 7.795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.617 >= min_delta = 0.01. New best score: 7.178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.308 >= min_delta = 0.01. New best score: 6.869\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.123 >= min_delta = 0.01. New best score: 6.747\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.184 >= min_delta = 0.01. New best score: 6.563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.148 >= min_delta = 0.01. New best score: 6.415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.181 >= min_delta = 0.01. New best score: 6.233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.198 >= min_delta = 0.01. New best score: 6.035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.046 >= min_delta = 0.01. New best score: 5.990\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.028 >= min_delta = 0.01. New best score: 5.961\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.011 >= min_delta = 0.01. New best score: 5.950\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.016 >= min_delta = 0.01. New best score: 5.934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.023 >= min_delta = 0.01. New best score: 5.911\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 15 records. Best score: 5.911. Signaling Trainer to stop.\n"
     ]
    }
   ],
   "source": [
    "accelerator=\"gpu\"\n",
    "devices=1\n",
    "max_epochs=1000\n",
    "precision = 32\n",
    "\n",
    "rootdir = \"/disk/scratch/s2208943/results/new_tests/evid_ssn\"\n",
    "final_dir = rootdir\n",
    "checkpoint_callback = ModelCheckpoint(final_dir, save_top_k=-1, monitor=\"val_loss\", every_n_epochs=5)\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.01, patience=15, verbose=\"False\", mode=\"min\", check_finite=True)\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    accelerator=accelerator,\n",
    "    devices=devices,\n",
    "    max_epochs=max_epochs,\n",
    "    precision=precision,\n",
    "    default_root_dir=final_dir\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader,)# ckpt_path='/disk/scratch_big/s2208943/results/new_tests/epoch=15-step=2192.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bd618bc-d8ae-40df-8af3-bc913acfed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckpt = '/disk/scratch/s2208943/results/new_tests/evid_ssn/epoch=39-step=2880-v1.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96490bb1-3b6d-4350-a353-52b3e0bdfc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /disk/scratch_big/s2208943/results/new_tests/evid_ssn/epoch=24-step=1800-v5.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /disk/scratch_big/s2208943/results/new_tests/evid_ssn/epoch=24-step=1800-v5.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6266d6853bf54fd6962e05a32c0a5578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        val_loss            5.9396748542785645\n",
      "    val_metric_epoch        0.40767714381217957\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_metric_epoch': 0.40767714381217957, 'val_loss': 5.9396748542785645}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, val_dataloader, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcece1f-ce4f-49ed-b118-e2def5365aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d796c-8f63-423e-bbdf-b6e63158a164",
   "metadata": {},
   "source": [
    "### Loading the dataset in 3D for analysis\n",
    "now each sample from the 3D dataset we can treat as a batch, and is a single image sample. Note that each batch may not be the same size now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eefcd38-7925-42b1-9098-5fd31bc4bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load datasets\n",
    "# # this step is quite slow, all the data is being loaded into memory\n",
    "# # BIG NOTE\n",
    "# # during evaluating the modle, we set is_3d false here to avoid doing the z cropping (which can cause nans)\n",
    "# # and instead we run the whole brain scan through and it gives us a good prediction over all. Nice.\n",
    "# datasets_domains_3d = [MRISegmentation3DDataset(root_dir, domain, transforms=get_transforms(is_3D=False)) for domain in domains]\n",
    "\n",
    "\n",
    "# # split into train, val test datasets\n",
    "# datasets_3d = [train_val_test_split(dataset, validation_proportion, test_proportion, seed) for dataset in datasets_domains_3d]\n",
    "\n",
    "# # concat the train val test datsets\n",
    "# train_dataset_3d = ConcatDataset([ds[0] for ds in datasets_3d])\n",
    "# val_dataset_3d = ConcatDataset([ds[1] for ds in datasets_3d])\n",
    "# test_dataset_3d = ConcatDataset([ds[2] for ds in datasets_3d])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a36507a-6704-40a3-8acb-0ae57e78b1a7",
   "metadata": {},
   "source": [
    "### generating the samples procedure, only do this once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c73cc7-377b-4f39-894c-08f838b1fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c27a99-98ba-419c-9d75-2aaf7a8a28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs3d = []\n",
    "ys3d = []\n",
    "for x, y in val_dataset_3d:\n",
    "    xs3d.append(x)\n",
    "    ys3d.append(y.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd78cc3-7af4-41c7-bf7e-300a309d37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs3d[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b341611-1644-4265-a480-8f2670cc662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/s2208943/ipdis/results/final_models/\"\n",
    "#folders = [\"ensemble\" + ndigit(2, x+1) + \"/\" for x in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb37fa-5044-40db-b122-797b37aa2e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = \"/disk/scratch/s2208943/results/final_models/\"\n",
    "f = \"ssn_hpt10/\"\n",
    "ckpts = sorted([c for c in os.listdir(root+f) if \"epoch\" in c])\n",
    "c = ckpts[-1]\n",
    "# model = StandardLitModelWrapper.load_from_checkpoint(root + f + c, model=model_raw, loss=loss, \n",
    "#                                 logging_metric=SsnDiceMetricWrapper,\n",
    "#                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1d305-7f4f-4569-ae74-ad9e5be12b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samples(xs3dQ, ys3dQ, model=model):\n",
    "    samples3d = []\n",
    "    model = model.cuda()\n",
    "    for i in tqdm(range(len(ys3dQ)), position=0, leave=True, ncols=150):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(xs3dQ[i].swapaxes(0,1).cuda())\n",
    "            samples = fixed_re_parametrization_trick(out['distribution'], 20).cpu()\n",
    "            samples3d.append(samples)\n",
    "            \n",
    "    return samples3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0b039-8ed3-4c64-9a63-955309e8e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples3d = gen_samples(xs3d, ys3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885d7b4-0d67-4131-9555-1a7204d25f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "means3d = [torch.mean(samples3d[i], dim=0) for i in range(len(samples3d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5adb6-812d-459e-83f4-9561d114f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples3d[0].shape, means3d[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0420ccd-f4a6-4cfb-9a54-29d2bfe9dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e4575-9192-4157-8c92-630ec95521e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples3d), len(samples3d[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07067153-3e75-43b1-9395-9b807a2d2407",
   "metadata": {},
   "source": [
    "### generating the 2D samples of the first batch only, for the pictures! (and compute computation time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad1246-ae93-4d88-b732-ac28566a5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a4c2d-9306-4fba-924f-7484cc8b3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for x, y in val_dataloader:\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6068f10c-9492-40eb-b9bd-11d6103dcd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2d_onebatch = []\n",
    "model = model.cuda()\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(xs[0].cuda())\n",
    "    samples = fixed_re_parametrization_trick(out['distribution'], 20).cpu()\n",
    "    samples2d_onebatch = samples\n",
    "        \n",
    "elapsed = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed283d7-9471-4ec2-838f-79776ded17b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2d = samples2d_onebatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42435fbb-8345-40a2-b561-95396d1da409",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"elapsed time (s): \", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35dbfe6-e42e-443a-9eea-89261d0676bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples2d_onebatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1e9b97-2c36-4483-97d9-33e8a1dbfb5b",
   "metadata": {},
   "source": [
    "### fixed compute section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68765d44-e637-4bc0-ba76-6e45999008e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ssn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77217f7-8e76-4b23-ac24-9b56719c99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir(\"results/\" + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4fa530-0ae6-4036-8a31-f1e4692d55ae",
   "metadata": {},
   "source": [
    "### Generating uncertainty maps for a batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad86065-5963-4453-b9c9-31a589732672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_map_from_samples(samples):\n",
    "    \"samples is of shape samples, batch size, channels, image dims  [s, b, c *<dims>]\"\n",
    "    probs = torch.nn.functional.softmax(samples, dim=2)\n",
    "    pic = torch.mean(probs, dim=0)\n",
    "    ent_map = torch.sum(-pic * torch.log(pic+1e-30), dim=1)\n",
    "    \n",
    "    return ent_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0cd61-dbeb-46cf-a2c3-792c821fbc1a",
   "metadata": {},
   "source": [
    "### Creating the entropy map for the 2D samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959f47b5-6bca-4598-8fae-de02ef84b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456aa157-f1b2-4f17-9b1f-289c2ce0f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent2dbatch = entropy_map_from_samples(samples2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f61a3-6a25-4366-9e6d-56a89176e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent2dbatch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad21bb-5eea-4276-8fda-23a658d7a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(fname):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/\"+model_name + \"/\" + fname, bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2358cc1-835f-4bbc-83f7-43ccc30b3e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ddc75-c5ba-42d5-b040-e3274682a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "showpred = samples2d.mean(dim=0).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf6c71-7ae1-41c4-9179-daca725de129",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(showpred[5], cmap='gray');\n",
    "plt.axis('off');\n",
    "save(\"pred5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051f8ad-5a16-447a-9e07-dfff2de639ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(showpred[20], cmap='gray');\n",
    "plt.axis('off');\n",
    "save(\"pred20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a81c3c-02e8-41dd-a4e1-214e131bb183",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(showpred[58], cmap='gray');\n",
    "plt.axis('off');\n",
    "save(\"pred58\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d820b79-d8bd-4f34-88b2-b01b8de24970",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(showpred[37], cmap='gray');\n",
    "plt.axis('off');\n",
    "save(\"pred37\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e349a-ff4f-4f04-b819-3b4c10502806",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(showpred[62], cmap='gray');\n",
    "plt.axis('off');\n",
    "save(\"pred62\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639dc5e-1adc-41f1-b536-7337dd5e39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(showpred[49], cmap='gray');\n",
    "plt.axis('off');\n",
    "save(\"pred49\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c80183-f809-4c16-9a6e-7d3a6a3aed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(showpred[50], cmap='gray');\n",
    "plt.axis('off');\n",
    "save(\"pred50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a2b05-b805-4da4-8491-13c291421d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(showpred[23], cmap='gray');\n",
    "plt.axis('off');\n",
    "save(\"pred23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d314db-4a64-49a1-885c-ed44a94adbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(showpred[30], cmap='gray');\n",
    "plt.axis('off');\n",
    "save(\"pred30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f1bcb6-59cb-4b64-ac29-2316a80fece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(showpred[48], cmap='gray');\n",
    "plt.axis('off');\n",
    "save(\"pred48\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420ce53-f547-4209-b830-83a88574c028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092815a9-8bf2-418d-8327-474c2ed39a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e3450-c4fa-4d3a-97a0-1f8d72fc30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ent2dbatch[5], vmin=0, vmax=0.7);\n",
    "plt.axis('off');\n",
    "save(\"img5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c4ccaf-1d69-4278-8848-ac4f75ce4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ent2dbatch[20], vmin=0, vmax=0.7);\n",
    "plt.axis('off');\n",
    "save(\"img20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c780e8-8da4-4d4d-aa5c-cac7173866dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ent2dbatch[58], vmin=0, vmax=0.7);\n",
    "plt.axis('off');\n",
    "save(\"img58\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da92762-476d-4ed4-ba58-11a291c1baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ent2dbatch[37], vmin=0, vmax=0.7);\n",
    "plt.axis('off');\n",
    "save(\"img37\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbddb9a-fe53-421e-af6e-1d30b4178540",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ent2dbatch[62], vmin=0, vmax=0.7);\n",
    "plt.axis('off');\n",
    "save(\"img62\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4804ce1-a6c0-4c82-ba4c-2edb7190d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ent2dbatch[49], vmin=0, vmax=0.7);\n",
    "plt.axis('off');\n",
    "save(\"img49\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efce6a-87eb-4dc9-b9f6-db40aeff8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ent2dbatch[50], vmin=0, vmax=0.7);\n",
    "plt.axis('off');\n",
    "save(\"img50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1601e-dbd6-450d-82b7-e9b071425cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ent2dbatch[23], vmin=0, vmax=0.7);\n",
    "plt.axis('off');\n",
    "save(\"img23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a7211-21df-40bd-a78f-f59f10773a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ent2dbatch[30], vmin=0, vmax=0.7);\n",
    "plt.axis('off');\n",
    "save(\"img30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c424bae-014b-4749-b4d9-f536b622bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ent2dbatch[48], vmin=0, vmax=0.7);\n",
    "plt.axis('off');\n",
    "save(\"img48\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5da7b-0249-4a5a-bbc1-06a47b435118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(30,20))\n",
    "# ids = 5, 20, 58, 37, 62, 49, 50, 23, 30, 48\n",
    "# count = 0\n",
    "# for i in range(64):\n",
    "#     if torch.sum(ys[0][i]) > 0:\n",
    "#         plt.subplot(6, 12, count+1)\n",
    "#         plt.imshow(ys[0][i], cmap='gray')\n",
    "#         plt.title(i)\n",
    "#         plt.axis('off')\n",
    "#         plt.subplot(6, 12, count+2)\n",
    "#         plt.imshow(ent2dbatch[i], vmin=0, vmax=0.7)\n",
    "#         plt.title(i)\n",
    "#         plt.axis('off')\n",
    "#         count += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1f57b-fdd2-48d3-a1ad-efccdc72f1ab",
   "metadata": {},
   "source": [
    "interesting images: 5, 20, 58, 37, 62, 49, 50, 23, 30, 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff34980-b224-41ad-9270-b168db02abab",
   "metadata": {},
   "source": [
    "### How do average dice and best dice improve with samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563d234-1f7d-432e-b695-9076a575e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples3d), len(samples3d[0]), len(ys3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75114bcf-814a-4fa3-8f64-dd2b6dc10f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(y_pred, y_true):\n",
    "    y_pred = torch.nn.functional.softmax(y_pred, dim=1).argmax(dim=1)\n",
    "    #print(y_pred.shape, y_true.shape)\n",
    "    denominator = torch.sum(y_pred) + torch.sum(y_true)\n",
    "    numerator = 2. * torch.sum(torch.logical_and(y_pred, y_true))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45a461-30dd-48d7-9348-68f1091a5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the dice per sample, per individual\n",
    "dices3d = []\n",
    "for ind in tqdm(range(len(samples3d)), position=0, leave=True, ncols=150):\n",
    "    sample_dices = []\n",
    "    for s in range(len(samples3d[ind])):\n",
    "        y_hat = samples3d[ind][s]\n",
    "        y = ys3d[ind]\n",
    "        sample_dices.append(dice(y_hat, y))\n",
    "    dices3d.append(sample_dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7265f5-22d0-4aaa-9bc9-24297301257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_alldice3d = torch.stack([torch.Tensor(ds) for ds in dices3d], dim=0).swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062a4f6-00ae-4fd0-9ab2-5f79608f2890",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_alldice3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673da66b-632f-48d6-b093-1b0f17470f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean and best dice as the number of samples increases from 1 to 20:\n",
    "samples_vs_dices_mean = []\n",
    "samples_vs_dices_best = []\n",
    "for i in range(20):\n",
    "    selected_samples_dices = tensor_alldice3d[0:i+1]\n",
    "    mean_dice = torch.mean(selected_samples_dices, dim=0)\n",
    "    best_dice = torch.max(selected_samples_dices, dim=0)[0]\n",
    "    \n",
    "    samples_vs_dices_mean.append(mean_dice.mean())\n",
    "    samples_vs_dices_best.append(best_dice.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83727fe2-f817-4eab-96f9-3eaddbd34e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dice.mean(),mean_dice.std(), best_dice.mean(), best_dice.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd163bf-600f-436f-84df-152949a66473",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_vs_dices_mean[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16779ba-772a-4332-b1eb-2d5b3e19af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b301281-cd16-4153-b198-35218444c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.arange(0, len(samples_vs_dices_mean), 1), samples_vs_dices_best)\n",
    "plt.ylim(0.55, 0.75)\n",
    "plt.xticks(torch.arange(0, 20, 2));\n",
    "plt.xlabel(\"Num. Samples\")\n",
    "plt.ylabel(\"Dice\");\n",
    "save(\"samples_v_dice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9edb1-5001-4271-868d-bbe6f050dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.Tensor(samples_vs_dices_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb438283-9e6f-4f33-8fc2-7a60765ffc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort in order of quality\n",
    "order = torch.sort(torch.median(tensor_alldice3d, dim=0)[0])[1]\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.boxplot(tensor_alldice3d.T[order]);\n",
    "plt.ylim(0, 1);\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "plt.ylabel(\"Dice\")\n",
    "plt.xlabel(\"Individuals\")\n",
    "save(\"sample_diversity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b4e47f-93c7-4486-8535-cfc1be3f5bad",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701ad5b-f173-4e04-b623-07c335e6a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_in_bin(value):\n",
    "    return torch.round(value, decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470dcd3f-8163-4e90-8935-d639ff57a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_average(value, n, G):\n",
    "    return value / n + ((n-1) / n) * G\n",
    "\n",
    "def batch_rolling_average(values, n, G):\n",
    "    \"\"\"\n",
    "    assumes all batches but the last batch are the same size\n",
    "    \"\"\"\n",
    "    return values.sum() / (values.shape[0]*n) + ((n-1) / n) * G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb7d42-4ea5-40d5-94a1-fd76e121c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af3e50c-5e83-47be-a8da-ecd1b7d2123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess bin counts of p y = 1\n",
    "bins = 10 + 1 # for the 0 bin\n",
    "bin_batch_accuracies = [[] for b in range(bins)]\n",
    "bin_batch_confidences = [[] for b in range(bins)]\n",
    "bin_batch_sizes = [[] for b in range(bins)]\n",
    "bin_counts = [0 for b in range(bins)]\n",
    "for batch_idx in tqdm(range(len(ys3d)), ncols=150, position=0, leave=True): # skip the last batch with a different shape\n",
    "    batch_t = ys3d[batch_idx].squeeze()\n",
    "    batch_samples = samples3d[batch_idx]\n",
    "    \n",
    "    if batch_t.shape[0] < 10:\n",
    "        continue # skip last batch if it is very small.\n",
    "    \n",
    "    # get probabilities\n",
    "    probs = torch.nn.functional.softmax(batch_samples, dim=2)\n",
    "    p1s = probs[:,:,1]\n",
    "    \n",
    "    # split into bins\n",
    "    bin_ids = place_in_bin(p1s)\n",
    "    \n",
    "    # compute counts\n",
    "    for i in range(bins):\n",
    "        is_in_bin = (bin_ids == (i / 10))\n",
    "        # print(is_in_bin.shape)\n",
    "        # print(batch_t.shape)\n",
    "        \n",
    "        # number of elements in each bin\n",
    "        num_elem = torch.sum(is_in_bin).item()\n",
    "        if num_elem == 0:\n",
    "            print(\"zero\")\n",
    "        \n",
    "        # number of predictions = to class 1\n",
    "        c1_acc = batch_t.expand(p1s.shape)[is_in_bin].sum() / num_elem\n",
    "        \n",
    "        if torch.isnan(c1_acc):\n",
    "            print(\"acc_nan\")\n",
    "        \n",
    "        # average confidence of values in that bin\n",
    "        c1_conf = p1s[is_in_bin].mean()\n",
    "        \n",
    "        if torch.isnan(c1_conf):\n",
    "            print(\"conf_nan\")\n",
    "        \n",
    "        bin_batch_accuracies[i].append(c1_acc)\n",
    "        bin_batch_confidences[i].append(c1_conf)\n",
    "        bin_batch_sizes[i].append(num_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f4675f-380c-4467-8584-dffec3119aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_sizes = [torch.Tensor(bbs).sum() for bbs in bin_batch_sizes]\n",
    "bin_accuracies = [torch.Tensor([bin_batch_accuracies[i][j] * bin_batch_sizes[i][j] / bin_sizes[i] for j in range(len(bin_batch_accuracies[i]))]).sum().item() for i in range(len(bin_sizes))]\n",
    "bin_confidences = [torch.Tensor([bin_batch_confidences[i][j] * bin_batch_sizes[i][j] / bin_sizes[i] for j in range(len(bin_batch_confidences[i]))]).sum().item() for i in range(len(bin_sizes))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698135c-d73e-49fe-bff8-88bce783078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2dc9d-ee42-4c74-9aae-38f56b2df65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e65fc-6c4d-4cbe-9f5d-10b2d5958796",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = torch.sum(torch.Tensor(bin_sizes)[1:])\n",
    "ece = torch.sum( (torch.Tensor(bin_sizes)[1:]/ total_size) * (torch.abs(torch.Tensor(bin_accuracies)[1:] - torch.Tensor(bin_confidences)[1:])))\n",
    "print(\"EXPECTED CALIBRATION ERROR: note we skip the first bin due to its size\", ece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46137c04-4841-4de5-89e0-1262049d1dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "looking at just the class one calibration is useful due to the extreme lack of class 1 in the image, this gives us a better idea of how the model is doing\n",
    "and when optimizing calibration I should be optimizing this.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64da5cb-c64d-4515-a9f6-55d38ba4bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bin_confidences, bin_accuracies)\n",
    "plt.plot([0,1],[0,1]);\n",
    "plt.xlabel(\"Confidence\")\n",
    "plt.ylabel(\"Accuracy\");\n",
    "save(\"calibration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82253e-ce51-4258-b10f-6edc6546a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exd = batch_t.expand(p1s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8bcce-4cc0-42b8-b708-e29d779764d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar([i for i in range(bins)], torch.Tensor(bin_sizes), align='center')\n",
    "# plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3267eb7-c082-4728-a01f-d68a26a6a90c",
   "metadata": {},
   "source": [
    "### Compute the PavPU metrics cause I'm not convinced my previous code in original ssn notebook was incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b72b7f7-dc24-44ff-85e4-cd99a6ba6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to do it per patch, but take the average accuracy per patch (perhaps I should qc average dice as well, best dice, worst dice.\n",
    "uncetainty_thresholds = torch.arange(0, 0.7, 0.01)\n",
    "accuracy_threshold = 0.9\n",
    "window = 8\n",
    "stride = 8\n",
    "n_acs = [[] for i in range(len(uncetainty_thresholds))]\n",
    "n_aus = [[] for i in range(len(uncetainty_thresholds))]\n",
    "n_ics = [[] for i in range(len(uncetainty_thresholds))]\n",
    "n_ius = [[] for i in range(len(uncetainty_thresholds))]\n",
    "\n",
    "for batch_idx in tqdm(range(len(ys3d)), ncols=150, position=0, leave=True): # skip the last batch with a different shape\n",
    "    batch_t = ys3d[batch_idx].squeeze()\n",
    "    batch_samples = samples3d[batch_idx]\n",
    "    ent = entropy_map_from_samples(batch_samples)\n",
    "    \n",
    "    # get probabilities\n",
    "    probs = torch.nn.functional.softmax(batch_samples, dim=2)\n",
    "    pred_classes = probs.argmax(dim=2)\n",
    "    confidences = probs.max(dim=2)[0]\n",
    "    \n",
    "    # get average accuracy of each sample\n",
    "    # or I could treat each patch of each sample as a separate thing but that is not what I am doing here.\n",
    "    avg_accuracy = ((batch_t.expand(pred_classes.shape) == pred_classes) * 1.).mean(dim=0)\n",
    "    \n",
    "    # unroll predictions and targets and entropy\n",
    "    t_unrolled = batch_t.unfold(-2, window, stride).unfold(-1, window, stride).reshape(-1, window, window)\n",
    "    accuracy_unrolled = avg_accuracy.unfold(-2, window, stride).unfold(-1, window, stride).reshape(-1, window, window)\n",
    "    ent_unrolled = ent.unfold(-2, window, stride).unfold(-1, window, stride).reshape(-1, window, window)\n",
    "    \n",
    "    accurate_patches = accuracy_unrolled > 0.9\n",
    "    \n",
    "    # for each uncertainty threshold, compute the 4 numbers\n",
    "    for i, uncert_t in enumerate(uncetainty_thresholds):\n",
    "        uncertain_patches = ent_unrolled > uncert_t\n",
    "        \n",
    "        n_acs[i].append(torch.sum(torch.logical_and(accurate_patches, ~uncertain_patches)))\n",
    "        n_aus[i].append(torch.sum(torch.logical_and(accurate_patches, uncertain_patches)))\n",
    "        n_ics[i].append(torch.sum(torch.logical_and(~accurate_patches, ~uncertain_patches)))\n",
    "        n_ius[i].append(torch.sum(torch.logical_and(~accurate_patches, uncertain_patches)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842bfc5-4e7c-412d-8c4e-1fcca9038a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_acs_t = [torch.Tensor(n_acs[i]).sum() for i in range(len(uncetainty_thresholds))]\n",
    "n_aus_t = [torch.Tensor(n_aus[i]).sum() for i in range(len(uncetainty_thresholds))]\n",
    "n_ics_t = [torch.Tensor(n_ics[i]).sum() for i in range(len(uncetainty_thresholds))]\n",
    "n_ius_t = [torch.Tensor(n_ius[i]).sum() for i in range(len(uncetainty_thresholds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d8354-075c-4d75-8d8f-0d10f280dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_acs = [n_acs_t[i] / (n_acs_t[i] + n_ics_t[i]) for i in range(len(uncetainty_thresholds))]\n",
    "p_aus = [n_ius_t[i] / (n_ius_t[i] + n_ics_t[i]) for i in range(len(uncetainty_thresholds))]\n",
    "pavpu = [(n_acs_t[i] + n_ius_t[i]) / (n_ius_t[i] + n_ics_t[i] + n_aus_t[i] + n_acs_t[i]) for i in range(len(uncetainty_thresholds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57066b-a357-4b0a-976e-f3f6054c8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(uncetainty_thresholds, p_acs, c='g')\n",
    "plt.xlabel(\"τ\")\n",
    "plt.ylabel(\"p(acc|cert)\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(uncetainty_thresholds, p_aus, c='g')\n",
    "plt.ylabel(\"p(uncert|inacc)\")\n",
    "plt.xlabel(\"τ\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(uncetainty_thresholds, pavpu, c='g')\n",
    "plt.ylabel(\"PAVPU\")\n",
    "plt.xlabel(\"τ\")\n",
    "save(\"pavpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c11d8-99f7-4e5a-9d6d-dbeb0b854529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncetainty_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026cbc7b-f3c2-4d31-9322-26cf03e966ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=5\n",
    "uncetainty_thresholds[i], p_acs[i], p_aus[i], pavpu[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004fcdbf-628d-4311-b9e6-232f1acc14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent_thresh = uncetainty_thresholds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b40ad5-fb24-4a3f-ba54-96bf93e59a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_thresh = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1231f0-976a-4f92-8694-54aa43f4040c",
   "metadata": {},
   "source": [
    "### Quality Control in 3D - vcc corr-coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aff01c-1a87-4a13-9d44-e5ff18ed0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate entropy maps per individual\n",
    "ind_ent_maps = [entropy_map_from_samples(samples3d[i]) for i in range(len(ys3d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b02f2b6-e1f4-4105-8397-d3d3d8b8158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VVC(v):\n",
    "    v = torch.nn.functional.softmax(v, dim=2)\n",
    "    return torch.std(v) / torch.mean(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98288919-655b-4755-baee-e0d21570bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vvcs = [VVC(samples3d[i]) for i in range(len(ys3d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d79cc54-ff35-49a6-bd13-b9db27c5c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vvcs_ent = [VVC(ind_ent_maps[i]) for i in range(len(ys3d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570f2fe-9c9f-40c7-8c46-9a00241289a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = torch.median(tensor_alldice3d, dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb8c945-5a2c-4de6-b3f5-714021037b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"correlation coefficient: \", torch.corrcoef(torch.stack([torch.Tensor(vvcs), medians]))[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45987bb9-b302-4424-bdd0-9a3d99ec4e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(vvcs, medians)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c086f0c3-2597-4546-998c-551d419a1bb6",
   "metadata": {},
   "source": [
    "### TP FP TN FN distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a890c-ffdf-44c2-8cb8-1cdb2e98cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tps = []\n",
    "#all_tns = []\n",
    "all_fps = []\n",
    "all_fns = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(ys3d)), position=0, leave=True, ncols=150):\n",
    "        samples = samples3d[i]\n",
    "        mean = means3d[i]\n",
    "        ent = ind_ent_maps[i].view(-1)\n",
    "        mean_class = mean.argmax(dim=1).view(-1)\n",
    "        y = ys3d[i]\n",
    "        x = xs3d[i].swapaxes(0,1)\n",
    "        y_flat = y.view(-1)\n",
    "        \n",
    "        tp_loc = torch.where(torch.logical_and(y_flat == 1, mean_class == 1))[0]\n",
    "        #tn_loc = torch.where(torch.logical_and(torch.logical_and(y_flat == 0, mean_class == 0), x[:,1].reshape(-1) == 1))[0]\n",
    "        fp_loc = torch.where(torch.logical_and(y_flat == 0, mean_class == 1))[0]\n",
    "        fn_loc = torch.where(torch.logical_and(torch.logical_and(y_flat == 1, mean_class == 0), x[:,1].reshape(-1) == 1))[0]\n",
    "        # print(tp_loc.shape)\n",
    "        # print(ent.view(-1).shape)\n",
    "        \n",
    "        all_tps.append(ent[tp_loc])\n",
    "        #all_tns.append(ent[tn_loc])\n",
    "        all_fps.append(ent[fp_loc])\n",
    "        all_fns.append(ent[fn_loc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f8514-a620-4f5f-bb08-92743149f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tps = torch.cat(all_tps)\n",
    "#tns = torch.cat(all_tns)\n",
    "fps = torch.cat(all_fps)\n",
    "fns = torch.cat(all_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed3504-fd2d-406e-9e2d-643c866d400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tps.shape, fps.shape, fns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0c8c8-ba09-4475-8067-47c062f53600",
   "metadata": {},
   "outputs": [],
   "source": [
    "tps.mean(), fps.mean(), fns.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73400938-7bed-48b2-b296-1fd33921fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tps, bins=100, color='r');\n",
    "plt.ylabel(\"Voxels per Bin\")\n",
    "#plt.ylim((0, 350000))\n",
    "plt.xlabel(\"$H$\")\n",
    "save(\"tps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adca605-27e2-4555-99d2-7624c528b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fps, bins=100, color='r');\n",
    "plt.ylabel(\"Voxels per Bin\")\n",
    "#plt.ylim((0, 50000))\n",
    "plt.xlabel(\"$H$\")\n",
    "save(\"fps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ad20d-7dfc-4268-9587-e535685fbfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fns, bins=100, color='r');\n",
    "plt.ylabel(\"Voxels per Bin\")\n",
    "#plt.ylim((0, 155000))\n",
    "plt.xlabel(\"$H$\")\n",
    "save(\"fns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda6138-4c3c-49c5-b5a9-831b793905e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac1e7a-70c0-4196-998b-377dea437e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = -1\n",
    "ntps = len(tps)\n",
    "nfns = len(fns)\n",
    "nfps = len(fps)\n",
    "data = {\"label\":[\"TP\" for _ in range(ntps)][0:j] + [\"FN\" for _ in range(nfns)][0:j] + [\"FP\" for _ in range(nfps)][0:j], \"ent\": torch.cat([tps[0:j], fns[0:j], fps[0:j]]).numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d0860-2923-4402-b8fd-12967acbe82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(20, 5))\n",
    "sns.violinplot(x=\"label\", y=\"ent\", data=data, linewidth=0.5, inner=None)\n",
    "plt.ylabel(\"$H$\")\n",
    "save(\"types_violin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55940009-4762-43e5-b23b-c63f3856e14c",
   "metadata": {},
   "source": [
    "### Missing lesions in 2D slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e73250f-6447-40e4-832a-c45370d1d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import kornia as K\n",
    "\n",
    "def create_random_labels_map(classes: int) -> Dict[int, Tuple[int, int, int]]:\n",
    "    labels_map: Dict[int, Tuple[int, int, int]] = {}\n",
    "    for i in classes:\n",
    "        labels_map[i] = torch.randint(0, 255, (3, ))\n",
    "    labels_map[0] = torch.zeros(3)\n",
    "    return labels_map\n",
    "\n",
    "def labels_to_image(img_labels: torch.Tensor, labels_map: Dict[int, Tuple[int, int, int]]) -> torch.Tensor:\n",
    "    \"\"\"Function that given an image with labels ids and their pixels intrensity mapping, creates a RGB\n",
    "    representation for visualisation purposes.\"\"\"\n",
    "    assert len(img_labels.shape) == 2, img_labels.shape\n",
    "    H, W = img_labels.shape\n",
    "    out = torch.empty(3, H, W, dtype=torch.uint8)\n",
    "    for label_id, label_val in labels_map.items():\n",
    "        mask = (img_labels == label_id)\n",
    "        for i in range(3):\n",
    "            out[i].masked_fill_(mask, label_val[i])\n",
    "    return out\n",
    "\n",
    "def show_components(img, labels):\n",
    "    color_ids = torch.unique(labels)\n",
    "    labels_map = create_random_labels_map(color_ids)\n",
    "    labels_img = labels_to_image(labels, labels_map)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,12))\n",
    "    \n",
    "    # Showing Original Image\n",
    "    ax1.imshow(img)\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.set_title(\"Orginal Image\")\n",
    "    \n",
    "    #Showing Image after Component Labeling\n",
    "    ax2.imshow(labels_img.permute(1,2,0).squeeze().numpy())\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title(\"Component Labeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ee4b0-b515-48a5-abfe-d74e04bdc210",
   "metadata": {},
   "outputs": [],
   "source": [
    "conncomp_outs = []\n",
    "\n",
    "for y in tqdm(ys3d, position=0, leave=True, ncols=150):\n",
    "    labels_out = K.contrib.connected_components(y.unsqueeze(1).type(torch.float32), num_iterations=150)\n",
    "    conncomp_outs.append(labels_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c044914-dba4-4b64-867a-2b028d517430",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_lesion_size_ent = []\n",
    "existing_lesion_size_ent = []\n",
    "proportion_missing_lesion_covered_ent = []\n",
    "missing_lesion_size_mean = []\n",
    "for batch in tqdm(range(len(ys3d)), position=0, leave=True, ncols=150):\n",
    "    for i in range(0, ys3d[batch].shape[0], 3):\n",
    "        conncomps = conncomp_outs[batch][i]\n",
    "        ent = ind_ent_maps[batch][i]\n",
    "        uncert = (ent > ent_thresh).type(torch.long)\n",
    "        mean = means3d[batch].argmax(dim=1)[i]\n",
    "        \n",
    "        ids = conncomps.unique()[1:]\n",
    "        for idx in ids:\n",
    "            cc = (conncomps == idx)\n",
    "            size = torch.sum(cc)\n",
    "            if torch.max(uncert * cc) > 0:\n",
    "                existing_lesion_size_ent.append(size)\n",
    "            else:\n",
    "                missing_lesion_size_ent.append(size)\n",
    "                \n",
    "            if torch.max(mean * cc) > 0:\n",
    "                proportion_missing_lesion_covered_ent.append(torch.sum(uncert * cc) / size)\n",
    "                missing_lesion_size_mean.append(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1cec5e-1fca-4ab7-9d46-4864154b1fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(missing_lesion_size_ent, bins=200);\n",
    "plt.xscale('log')\n",
    "plt.ylim(0, 500)\n",
    "plt.xlabel(\"Lesion Size\")\n",
    "plt.ylabel(\"Lesions per Bin\")\n",
    "save(\"missing_lesions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348a756-7af9-46a0-a97c-b8d13714f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"means and stds of existing and missing lesions in 2D\")\n",
    "existing = torch.Tensor(existing_lesion_size_ent)\n",
    "missing = torch.Tensor(missing_lesion_size_ent)\n",
    "existing.mean(), existing.std(), missing.mean(), missing.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e70195-1a01-41f1-a61e-fcb22e457ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total number of lesions missing in 2D\", len(missing_lesion_size_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd20d6-3483-4cc9-9a4a-0b54bf7b74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean proportion of missing lesion covered by uncertainty: \", torch.Tensor(proportion_missing_lesion_covered_ent).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01575d89-40c6-4a5d-bf72-6d1d9675436a",
   "metadata": {},
   "source": [
    "### How do uncertainty maps differ depending on the WMH damage values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef5fd37-cd7a-4a57-a4c1-971f5f0f5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_wmh_damage = []\n",
    "lhss = []\n",
    "rhss = []\n",
    "\n",
    "for i in range(len(ys3d)):\n",
    "    x = xs3d[i]\n",
    "    y = ys3d[i]\n",
    "\n",
    "    min_intensity = torch.min(x[0])\n",
    "    norm_max_intensity = torch.max(x[0][y == 1]) - min_intensity\n",
    "    nawm_range = (norm_max_intensity*0.5 + min_intensity, norm_max_intensity*0.75 + min_intensity)\n",
    "    wmh_voxels = x[0][y==1]\n",
    "    nawm_voxels = x[0][torch.logical_and(torch.logical_and(torch.logical_and(y==0, x[1] == 1), x[0] > nawm_range[0]), x[0] < nawm_range[1])]\n",
    "\n",
    "    I_wmh = torch.mean(wmh_voxels)\n",
    "    I_nawm = torch.mean(nawm_voxels)\n",
    "\n",
    "    wmh_volume = wmh_voxels.shape[0]\n",
    "    nawm_volume = nawm_voxels.shape[0]\n",
    "\n",
    "    lhs = ((I_wmh - I_nawm) / I_nawm)\n",
    "    rhs = (wmh_volume / (wmh_volume + nawm_volume))\n",
    "    damage = lhs * rhs\n",
    "\n",
    "    true_wmh_damage.append(damage)\n",
    "    lhss.append(lhs)\n",
    "    rhss.append(rhs)\n",
    "\n",
    "    print(\"I_wmh: \", I_wmh, \"I_nawm: \", I_nawm, \"wmh volume: \", wmh_volume, \"normal volume: \", nawm_volume, lhs, rhs, damage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac87832a-ee1b-4018-b509-783e43b38910",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean3ddice = tensor_alldice3d.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb05fe-a945-41c2-9487-f005926fbcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(true_wmh_damage, mean3ddice)\n",
    "plt.xscale('log')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Dice')\n",
    "plt.xlabel('WMH Damage Score')\n",
    "save(\"dice_v_damage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67906c-a135-4279-9eb4-5c202104fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ent_sums = [torch.sum(e) for e in ind_ent_maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52685f78-ccd0-49d0-a0c8-1ef11b5b8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(true_wmh_damage, ind_ent_sums, alpha=0.8, c='orange')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('sum($H$)')\n",
    "plt.xlabel('WMH Damage Score')\n",
    "save(\"ent_v_damage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04156c43-8c1f-48cb-aed0-6c8bf7df63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"correlation coefficient: \", torch.corrcoef(torch.stack([torch.Tensor(ind_ent_sums), torch.log10(torch.Tensor(true_wmh_damage))]))[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ba1471-d91d-460e-825e-146814475e47",
   "metadata": {},
   "source": [
    "### Domain Adaption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde8827-b6b6-46a0-9bc9-b6813a5e8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_all = [wmh_dir + d for d in [\"Singapore\", \"Utrecht\", \"GE3T\"]\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397b477-1bc2-4ee0-880d-c7792e892914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "# this step is quite slow, all the data is being loaded into memory\n",
    "datasets_domains_all = [MRISegmentation3DDataset(root_dir, domain, transforms=get_transforms(is_3D=False)) for domain in domains_all]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528806ef-f4d3-428b-ad9c-9823030b5603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess bin counts of p y = 1\n",
    "def ECE_domains(qys3d, qsamples3d):\n",
    "    bins = 10 + 1 # for the 0 bin\n",
    "    bin_batch_accuracies = [[] for b in range(bins)]\n",
    "    bin_batch_confidences = [[] for b in range(bins)]\n",
    "    bin_batch_sizes = [[] for b in range(bins)]\n",
    "    bin_counts = [0 for b in range(bins)]\n",
    "    for batch_idx in tqdm(range(0, len(qys3d)), ncols=150, position=0, leave=True): # skip the last batch with a different shape\n",
    "        batch_t = qys3d[batch_idx]\n",
    "        batch_samples = qsamples3d[batch_idx]\n",
    "\n",
    "        if batch_t.shape[0] < 10:\n",
    "            continue # skip last batch if it is very small.\n",
    "\n",
    "        # get probabilities\n",
    "        probs = torch.nn.functional.softmax(batch_samples, dim=2)\n",
    "        p1s = probs[:,:,1]\n",
    "\n",
    "        # split into bins\n",
    "        bin_ids = place_in_bin(p1s)\n",
    "\n",
    "        # compute counts\n",
    "        for i in range(bins):\n",
    "            is_in_bin = (bin_ids == (i / 10))\n",
    "            # print(is_in_bin.shape)\n",
    "            # print(batch_t.shape)\n",
    "\n",
    "            # number of elements in each bin\n",
    "            num_elem = torch.sum(is_in_bin).item()\n",
    "            if num_elem == 0:\n",
    "                print(\"zero\")\n",
    "\n",
    "            # number of predictions = to class 1\n",
    "            c1_acc = batch_t.expand(p1s.shape)[is_in_bin].sum() / num_elem\n",
    "\n",
    "            if torch.isnan(c1_acc):\n",
    "                print(\"acc_nan\")\n",
    "\n",
    "            # average confidence of values in that bin\n",
    "            c1_conf = p1s[is_in_bin].mean()\n",
    "\n",
    "            if torch.isnan(c1_conf):\n",
    "                print(\"conf_nan\")\n",
    "\n",
    "            bin_batch_accuracies[i].append(c1_acc)\n",
    "            bin_batch_confidences[i].append(c1_conf)\n",
    "            bin_batch_sizes[i].append(num_elem)\n",
    "\n",
    "    bin_sizes = [torch.Tensor(bbs).sum() for bbs in bin_batch_sizes]\n",
    "    bin_accuracies = [torch.Tensor([bin_batch_accuracies[i][j] * bin_batch_sizes[i][j] / bin_sizes[i] for j in range(len(bin_batch_accuracies[i]))]).sum().item() for i in range(len(bin_sizes))]\n",
    "    bin_confidences = [torch.Tensor([bin_batch_confidences[i][j] * bin_batch_sizes[i][j] / bin_sizes[i] for j in range(len(bin_batch_confidences[i]))]).sum().item() for i in range(len(bin_sizes))]\n",
    "    total_size = torch.sum(torch.Tensor(bin_sizes)[1:])\n",
    "    ece = torch.sum( (torch.Tensor(bin_sizes)[1:]/ total_size) * (torch.abs(torch.Tensor(bin_accuracies)[1:] - torch.Tensor(bin_confidences)[1:])))\n",
    "    print(\"EXPECTED CALIBRATION ERROR: note we skip the first bin due to its size\", ece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575455c6-b424-42fe-b548-deab288f401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the dice per sample, per individual\n",
    "def dices_ind(qsamples, qys):\n",
    "    qdices3d = []\n",
    "    for ind in tqdm(range(0, len(qsamples)), position=0, leave=True, ncols=150):\n",
    "        sample_dices = []\n",
    "        for s in range(len(qsamples[ind])):\n",
    "            y_hat = qsamples[ind][s]\n",
    "            y = qys[ind]\n",
    "            sample_dices.append(dice(y_hat, y))\n",
    "        qdices3d.append(sample_dices)\n",
    "        \n",
    "    qtensor_alldice3d = torch.stack([torch.Tensor(ds) for ds in qdices3d], dim=0).swapaxes(0,1)\n",
    "    qmean_dice = torch.mean(qtensor_alldice3d, dim=0)\n",
    "    qbest_dice = torch.max(qtensor_alldice3d, dim=0)[0]\n",
    "    \n",
    "    return qmean_dice.mean(), qmean_dice.std(), qbest_dice.mean(), qbest_dice.std()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b646125-68b8-4f06-be8b-c58066053a38",
   "metadata": {},
   "source": [
    "### domain 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2745766-b6d9-4174-8d91-79985f594d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wxs3d = []\n",
    "wys3d = []\n",
    "for i in range(0, len(datasets_domains_all[0]), 3):\n",
    "    x,y = datasets_domains_all[0][i]\n",
    "    wxs3d.append(x)\n",
    "    wys3d.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40af5a5-9967-4948-9234-89c632bf30c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsamples3d = gen_samples(wxs3d, wys3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0777c5-8313-4170-8477-8581750ad133",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECE_domains(wys3d, wsamples3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41801d8d-4013-4bc2-9e12-d1ae7dbcbbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices_ind(wsamples3d, wys3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfdf37e-9eae-4fcd-8c47-5b8230ba9ac1",
   "metadata": {},
   "source": [
    "### doamain 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38568fc3-47fa-45f0-b10c-d316b56b1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wxs3d = []\n",
    "wys3d = []\n",
    "for i in range(0, len(datasets_domains_all[1]), 3):\n",
    "    x,y = datasets_domains_all[0][i]\n",
    "    wxs3d.append(x)\n",
    "    wys3d.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c7c102-f0ae-49ed-8f58-5445a62bd89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsamples3d = gen_samples(wxs3d, wys3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb35cf-694e-4491-bbc6-35121a156b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECE_domains(wys3d, wsamples3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942ddb0-8c72-446d-b1fc-660dcba44cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices_ind(wsamples3d, wys3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430e3e2-7ce5-41f7-942e-448cbc9bf6bd",
   "metadata": {},
   "source": [
    "### domain 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c3c20-88b1-471c-802e-965617f2fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "wxs3d = []\n",
    "wys3d = []\n",
    "for i in range(0, len(datasets_domains_all[2]), 3):\n",
    "    x,y = datasets_domains_all[0][i]\n",
    "    wxs3d.append(x)\n",
    "    wys3d.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68399513-7a4e-4914-aeec-5ac759519764",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsamples3d = gen_samples(wxs3d, wys3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b479bb8-28d8-4577-95f7-4c5f8d1cb36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECE_domains(wys3d, wsamples3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67db2b-3436-4426-839f-23f2dabbfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dices_ind(wsamples3d, wys3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b471a9-48a7-400f-93eb-2bf9eea2c672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb2e5e-4864-4ec1-9457-37c84e93b31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25169e9a-40c7-4e89-90bb-492a01cb4b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45208407-5129-4746-92c5-a271cb5ed7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f91104-a0ac-4ecb-a3a3-37ad4aed4e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e2a7c-72bd-463b-8e36-ecbca77f69bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ae21f-5742-4328-a418-688da71d2c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f569dc1a-62ea-431d-98b3-f05c85dcef85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a243d6d-0385-4149-9f93-6c6a20986699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f072ddd-dd2f-4b36-8fbc-b6833cfe0ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1e8c6-c92e-4447-98aa-5b8efde1b892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4461cecd-5244-4030-941f-1d2a7042b0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0cf453-50e9-416a-8af6-a36b617d7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
